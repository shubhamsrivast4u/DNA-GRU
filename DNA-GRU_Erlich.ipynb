{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6c39185-fe0f-4126-99bc-2da49e794021",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1\n",
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 3080\n",
      "\n",
      "======================================================================\n",
      "DATASET: Erlich\n",
      "======================================================================\n",
      "  Label length:    152\n",
      "  Max deviation:   10\n",
      "  Max read length: 170\n",
      "  Target failure:  0.02%\n",
      "\n",
      "  Files:\n",
      "    Train: generated_data_corrected/binned_synthetic_erlich.txt\n",
      "    Eval:  Data/Erlich.txt\n",
      "  Output: Experiments/Erlich_ImprovedBiGRU_2026-01-14_12-56\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "LOADING DATA\n",
      "======================================================================\n",
      "\n",
      "Loading data from generated_data_corrected/binned_synthetic_erlich.txt...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47f29c5025654cdcba77a377bbdc0e6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing binned_synthetic_erlich.txt:   0%|          | 0/1500001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 1500000 clusters.\n",
      "\n",
      "Total clusters: 1,500,000\n",
      "Training:       1,470,000\n",
      "Validation:     30,000\n",
      "\n",
      "======================================================================\n",
      "MODEL ARCHITECTURE\n",
      "======================================================================\n",
      "Total parameters: 3,480,949 (3.48M)\n",
      "Embedding dim:    300\n",
      "Alignment:        128 filters\n",
      "Embedding:        500 filters\n",
      "GRU hidden:       300\n",
      "GRU layers:       2\n",
      "======================================================================\n",
      "\n",
      "Optimizer: Adam\n",
      "LR Schedule: Warmup 10 â†’ Cosine 5.00e-04 â†’ 5.00e-08\n",
      "Batch size: 200\n",
      "\n",
      "======================================================================\n",
      "TRAINING: Erlich\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03d218ed13584c1295420a600bf8b251",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/7350 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8780df1ad3f541848f7ebdf73f5f7044",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/120 | LR: 5.00e-04 | Train: 0.4289 | Val: 0.3995 | Time: 1436.4s âœ“ BEST (â†“inf)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e72c8962f9f6465bba9447995485f902",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/7350 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d965c2831dd343cbb1d7245417e23dd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   2/120 | LR: 5.00e-05 | Train: 0.3990 | Val: 0.3980 | Time: 1432.5s âœ“ BEST (â†“0.0015)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13c3b8575ef6406499a05ac8378774a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/7350 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "983544d5797d4c75a892be50e4ca7fd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   3/120 | LR: 1.00e-04 | Train: 0.3990 | Val: 0.3981 | Time: 1432.4s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ea99890901a4b90bec16aed36f40fb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/7350 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67431b061ada4192825e65ed858ac871",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   4/120 | LR: 1.50e-04 | Train: 0.3989 | Val: 0.3980 | Time: 1432.1s âœ“ BEST (â†“0.0001)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0420bf27abb9466eb3f1a5d7353fccb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/7350 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6bf72c9734747b9a86cd3a2fb7df51e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   5/120 | LR: 2.00e-04 | Train: 0.3989 | Val: 0.3979 | Time: 1432.0s âœ“ BEST (â†“0.0001)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7e9ce233beb4be9a21accf1e9641828",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/7350 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7885f7ea2a3744808cb105bf8ec8cb67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   6/120 | LR: 2.50e-04 | Train: 0.3988 | Val: 0.3983 | Time: 1432.1s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c878f11baeb4ba5842852eafd2b0c7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/7350 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b89b8a663ccb4c769785e2b5f0114e45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   7/120 | LR: 3.00e-04 | Train: 0.3988 | Val: 0.3980 | Time: 1431.8s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d16ec860dff48b2829674d9956ff780",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/7350 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07e8516003e24480a86dd2c360ef75c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   8/120 | LR: 3.50e-04 | Train: 0.3988 | Val: 0.3982 | Time: 1431.0s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "592e10a5137a4c5f959c30f8b50956b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/7350 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "883b23d4bbdd4f7e9d4011d64e309ef9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   9/120 | LR: 4.00e-04 | Train: 0.3988 | Val: 0.3978 | Time: 1431.5s âœ“ BEST (â†“0.0001)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18dfa49022004e05a8b379593f23a731",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/7350 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95b521679f1440269e59c437afacb801",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  10/120 | LR: 4.50e-04 | Train: 0.3988 | Val: 0.3981 | Time: 1431.0s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "444e44f43ed54940ae2ba93536e67589",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/7350 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec90f5acf3144c7581c186ba03cd7982",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  11/120 | LR: 5.00e-04 | Train: 0.3988 | Val: 0.3980 | Time: 1431.8s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49c18244079d4963bc744ca754ef2695",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/7350 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e280b179f9754fc384818fcb8efa00a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  12/120 | LR: 5.00e-04 | Train: 0.3987 | Val: 0.3980 | Time: 1431.7s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4ff30d27ff744fda689e4cf6a66a84a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/7350 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa85500f04e94b2c9ed3bee8c0630eb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  13/120 | LR: 5.00e-04 | Train: 0.3986 | Val: 0.3975 | Time: 1431.5s âœ“ BEST (â†“0.0003)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33bbf6637ee348a98e3782736ea4358f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/7350 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45978e8a8bd8482399f478e31984be94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  14/120 | LR: 5.00e-04 | Train: 0.3985 | Val: 0.3976 | Time: 1431.5s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0343d6063ba040b1922404c0f073111c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/7350 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3197122b79344858973a8c1b093b578a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  15/120 | LR: 4.99e-04 | Train: 0.3985 | Val: 0.3978 | Time: 1431.9s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6099838787f4500894c1b6e64fa5468",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/7350 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4d8d92c4ff94ff880d122d8e88ce471",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  16/120 | LR: 4.98e-04 | Train: 0.3984 | Val: 0.3976 | Time: 1431.8s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08d9df2c651c4bdfbac195b3c22eeffb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/7350 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c552056805ac490b939d82d0bfff2729",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  17/120 | LR: 4.97e-04 | Train: 0.3984 | Val: 0.3978 | Time: 1431.1s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6d62d6077404b86b9a5ed4599c51128",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/7350 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15c11e48042347988347c41d9392bc23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  18/120 | LR: 4.96e-04 | Train: 0.3983 | Val: 0.3977 | Time: 1431.9s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8911ef4cd19f49d9bb829965fd6dd535",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/7350 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e116732629e04e3094815350d9f94e95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  19/120 | LR: 4.95e-04 | Train: 0.3983 | Val: 0.3976 | Time: 1431.7s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0df98ca7b9b04b8985c68019b1042f43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/7350 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afe6c22da11047e5a87663db181ad228",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  20/120 | LR: 4.94e-04 | Train: 0.3983 | Val: 0.3974 | Time: 1431.1s âœ“ BEST (â†“0.0001)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eefb38d504954438b368c87676bf3092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/7350 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "153b468a7253421bb5e274e198aa5bc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  21/120 | LR: 4.92e-04 | Train: 0.3983 | Val: 0.3977 | Time: 1431.9s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29485ad83c964b26a0b8453c9132dbdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/7350 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eb3a49088e342119f65f5685fad52b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  22/120 | LR: 4.90e-04 | Train: 0.3983 | Val: 0.3978 | Time: 1431.8s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebe2d51d62b444f3997e62037eea4e9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/7350 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c071834438f4749b834fdebf7a7942a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  23/120 | LR: 4.88e-04 | Train: 0.3983 | Val: 0.3978 | Time: 1431.9s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1db63943c8248508027e5fe6c86733c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/7350 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd680d64091e43ec9320ce76aed95b16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  24/120 | LR: 4.85e-04 | Train: 0.3983 | Val: 0.3974 | Time: 1432.5s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "258e9c2f73a2486e83e5815d50b59fe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/7350 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e86c2818bfda47a394fecdf4b65ce35f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  25/120 | LR: 4.83e-04 | Train: 0.3983 | Val: 0.3975 | Time: 1431.8s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f793ca3df15a44bcabbf4b259199b440",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/7350 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af040c3e82494b28bde344d63970bd31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  26/120 | LR: 4.80e-04 | Train: 0.3982 | Val: 0.3975 | Time: 1431.4s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d069736e18f40a8af1f3fc5d9c676e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/7350 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0470503fd6624a93a5e236fe2dcb6dfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  27/120 | LR: 4.77e-04 | Train: 0.3982 | Val: 0.3975 | Time: 1431.4s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "309ed34b49d94d62a8bcc4141cffd6e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/7350 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99358624780c4a5a8e3acaa13618de92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  28/120 | LR: 4.74e-04 | Train: 0.3982 | Val: 0.3976 | Time: 1431.9s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63be56f192a84f949b40d35161dfc475",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/7350 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0906f35bf9ea40f4bd6f858d471bdfc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  29/120 | LR: 4.71e-04 | Train: 0.3982 | Val: 0.3978 | Time: 1432.2s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aeaf5125f5b424c8ef85592f24d8ee0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/7350 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc0797ea8a94445db4e6d9122f2c1788",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  30/120 | LR: 4.68e-04 | Train: 0.3982 | Val: 0.3976 | Time: 1431.6s\n",
      "\n",
      "Early stopping at epoch 30\n",
      "\n",
      "======================================================================\n",
      "Training Finished\n",
      "======================================================================\n",
      "Best val loss: 0.3974\n",
      "Models saved to: Experiments/Erlich_ImprovedBiGRU_2026-01-14_12-56/Models\n",
      "\n",
      "======================================================================\n",
      "EVALUATION: Erlich\n",
      "======================================================================\n",
      "\n",
      "âœ“ Loaded model from Experiments/Erlich_ImprovedBiGRU_2026-01-14_12-56/Models/best_model_Erlich.pth\n",
      "Loading data from Data/Erlich.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2602412/2205392058.py:704: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  inference_model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "933d76ece06b45248fcecfdeb707d5e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing Erlich.txt:   0%|          | 0/72001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 72000 clusters.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bbd5a3a481145c89743a24837a2acd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/180 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "RESULTS: Erlich\n",
      "======================================================================\n",
      "Total clusters:  72,000\n",
      "Failed clusters: 14\n",
      "Failure rate:    0.0194%\n",
      "Target:          0.02%\n",
      "======================================================================\n",
      "\n",
      "ðŸŽ‰ SUCCESS! Target achieved!\n",
      "\n",
      "Results saved to: Experiments/Erlich_ImprovedBiGRU_2026-01-14_12-56/Results\n",
      "\n",
      "======================================================================\n",
      "COMPLETE!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "##### # =============================================================================\n",
    "# IMPORTS\n",
    "# =============================================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "import math\n",
    "from datetime import datetime\n",
    "\n",
    "# =============================================================================\n",
    "# DEVICE SETUP\n",
    "# =============================================================================\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "if DEVICE == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# =============================================================================\n",
    "# ðŸ”§ CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "DATASET_NAME = \"Erlich\"  # Change to: \"Grass\", \"Organick\", \"Srinivasavaradhan\"\n",
    "\n",
    "DATASET_CONFIGS = {\n",
    "    \"Erlich\": {\n",
    "        \"label_len\": 152,\n",
    "        \"max_deviation\": 10,  # From paper\n",
    "        \"target_failure\": 0.02,\n",
    "    },\n",
    "    \"Grass\": {\n",
    "        \"label_len\": 117,\n",
    "        \"max_deviation\": 11,  # From paper\n",
    "        \"target_failure\": 0.66,\n",
    "    },\n",
    "    \"Organick\": {\n",
    "        \"label_len\": 110,\n",
    "        \"max_deviation\": 5,  # From paper\n",
    "        \"target_failure\": 0.17,\n",
    "    },\n",
    "    \"Srinivasavaradhan\": {\n",
    "        \"label_len\": 110,\n",
    "        \"max_deviation\": 10,  # From paper  \n",
    "        \"target_failure\": 14.58,\n",
    "    },\n",
    "}\n",
    "\n",
    "CONFIG = DATASET_CONFIGS[DATASET_NAME]\n",
    "LABEL_SEQ_LEN = CONFIG[\"label_len\"]\n",
    "MAX_DEVIATION = CONFIG[\"max_deviation\"]\n",
    "MAX_READ_LEN = LABEL_SEQ_LEN + MAX_DEVIATION + 8  # Extra buffer for safety\n",
    "MAX_CLUSTER_SIZE = 16  # Following DNAFormer paper\n",
    "\n",
    "# =============================================================================\n",
    "# TRAINING HYPERPARAMETERS\n",
    "# =============================================================================\n",
    "\n",
    "BATCH_SIZE = 200  # DNAFormer uses 64\n",
    "LEARNING_RATE = 5e-4  \n",
    "MIN_LR = 5e-8  # DNAFormer's ending LR\n",
    "WEIGHT_DECAY = 1e-5\n",
    "EPOCHS = 120\n",
    "WARMUP_EPOCHS = 10\n",
    "PATIENCE = 10\n",
    "\n",
    "# Model architecture\n",
    "EMBED_DIM = 300\n",
    "ALIGNMENT_FILTERS = 128  # Lighter than DNAFormer's 128\n",
    "EMBEDDING_FILTERS = 500  # Lighter than DNAFormer's 1024\n",
    "GRU_HIDDEN = 300\n",
    "GRU_LAYERS = 2\n",
    "DROPOUT = 0.1\n",
    "\n",
    "# =============================================================================\n",
    "# FILE PATHS\n",
    "# =============================================================================\n",
    "\n",
    "SYNTHETIC_DATA_DIR = Path(\"./generated_data_corrected\")\n",
    "REAL_DATA_DIR = Path(\"./Data\")\n",
    "\n",
    "\n",
    "TRAIN_FILE = SYNTHETIC_DATA_DIR / f\"binned_synthetic_{DATASET_NAME.lower()}.txt\"\n",
    "EVAL_FILE = REAL_DATA_DIR / f\"{DATASET_NAME}.txt\"\n",
    "\n",
    "\n",
    "# Generate a timestamp (e.g., 2023-10-27_14-30)\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n",
    "\n",
    "EXPERIMENT_DIR = Path(f\"./Experiments/{DATASET_NAME}_ImprovedBiGRU_{timestamp}\")\n",
    "WEIGHTS_DIR = EXPERIMENT_DIR / \"Models\"\n",
    "RESULTS_DIR = EXPERIMENT_DIR / \"Results\"\n",
    "WEIGHTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"DATASET: {DATASET_NAME}\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"  Label length:    {LABEL_SEQ_LEN}\")\n",
    "print(f\"  Max deviation:   {MAX_DEVIATION}\")\n",
    "print(f\"  Max read length: {MAX_READ_LEN}\")\n",
    "print(f\"  Target failure:  {CONFIG['target_failure']}%\")\n",
    "print(f\"\\n  Files:\")\n",
    "print(f\"    Train: {TRAIN_FILE}\")\n",
    "print(f\"    Eval:  {EVAL_FILE}\")\n",
    "print(f\"  Output: {EXPERIMENT_DIR}\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# =============================================================================\n",
    "# VOCABULARY\n",
    "# =============================================================================\n",
    "\n",
    "VOCAB = {'N': 0, 'A': 1, 'C': 2, 'G': 3, 'T': 4}\n",
    "PADDING_IDX = VOCAB['N']\n",
    "VOCAB_SIZE = len(VOCAB)\n",
    "INT_TO_CHAR = {i: c for c, i in VOCAB.items()}\n",
    "\n",
    "def encode_seq(seq: str, char_to_int: dict, max_len: int, padding_idx: int) -> list:\n",
    "    \"\"\"Encode DNA sequence to integers with padding\"\"\"\n",
    "    encoded = [char_to_int.get(c, padding_idx) for c in seq]\n",
    "    encoded = encoded[:max_len]\n",
    "    padded = encoded + [padding_idx] * (max_len - len(encoded))\n",
    "    return padded\n",
    "\n",
    "def decode_seq(tensor: torch.Tensor, int_to_char: dict) -> str:\n",
    "    \"\"\"Decode integer tensor to DNA sequence\"\"\"\n",
    "    if tensor.is_cuda:\n",
    "        tensor = tensor.cpu()\n",
    "    ints = tensor.numpy().tolist()\n",
    "    try:\n",
    "        first_pad = ints.index(PADDING_IDX)\n",
    "        ints = ints[:first_pad]\n",
    "    except ValueError:\n",
    "        pass\n",
    "    return \"\".join([int_to_char.get(i, '?') for i in ints])\n",
    "\n",
    "# =============================================================================\n",
    "# DATASET\n",
    "# =============================================================================\n",
    "\n",
    "class DnaClusterDataset(Dataset):\n",
    "    \"\"\"DNA Cluster Dataset - No filtering (filtering breaks results!)\"\"\"\n",
    "    \n",
    "    def __init__(self, filepath, max_cluster_size, max_read_len, label_seq_len, \n",
    "                 char_to_int, padding_idx):\n",
    "        self.max_cluster_size = max_cluster_size\n",
    "        self.max_read_len = max_read_len\n",
    "        self.label_seq_len = label_seq_len\n",
    "        self.char_to_int = char_to_int\n",
    "        self.padding_idx = padding_idx\n",
    "        self.labels = []\n",
    "        self.clusters = []\n",
    "        self._load_data(filepath)\n",
    "\n",
    "    def _load_data(self, filepath):\n",
    "        print(f\"Loading data from {filepath}...\")\n",
    "        try:\n",
    "            with open(filepath, 'r') as f:\n",
    "                content = f.read()\n",
    "        except FileNotFoundError:\n",
    "            print(f\"ERROR: File not found at {filepath}\")\n",
    "            raise\n",
    "        \n",
    "        blocks = content.split('\\n\\n')\n",
    "        for block in tqdm(blocks, desc=f\"Parsing {filepath.name}\"):\n",
    "            if not block.strip():\n",
    "                continue\n",
    "            lines = block.strip().split('\\n')\n",
    "            if len(lines) < 3:\n",
    "                continue\n",
    "            label_seq = lines[0]\n",
    "            reads = lines[2:]\n",
    "            if not reads or not label_seq:\n",
    "                continue\n",
    "            self.labels.append(label_seq)\n",
    "            self.clusters.append(reads)\n",
    "        print(f\"Successfully loaded {len(self.labels)} clusters.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label_str = self.labels[idx]\n",
    "        cluster_reads_str = self.clusters[idx]\n",
    "        \n",
    "        label_tensor = torch.tensor(\n",
    "            encode_seq(label_str, self.char_to_int, self.label_seq_len, self.padding_idx),\n",
    "            dtype=torch.long\n",
    "        )\n",
    "        \n",
    "        cluster_tensor = torch.full(\n",
    "            (self.max_cluster_size, self.max_read_len),\n",
    "            self.padding_idx,\n",
    "            dtype=torch.long\n",
    "        )\n",
    "        \n",
    "        random.shuffle(cluster_reads_str)\n",
    "        reads_to_process = cluster_reads_str[:self.max_cluster_size]\n",
    "        \n",
    "        for i, read_str in enumerate(reads_to_process):\n",
    "            cluster_tensor[i] = torch.tensor(\n",
    "                encode_seq(read_str, self.char_to_int, self.max_read_len, self.padding_idx),\n",
    "                dtype=torch.long\n",
    "            )\n",
    "        \n",
    "        return cluster_tensor, label_tensor\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL COMPONENTS (DNAFormer-inspired but lighter)\n",
    "# =============================================================================\n",
    "\n",
    "class DepthwiseSeparableConv1d(nn.Module):\n",
    "    \"\"\"Depthwise separable convolution (more efficient than standard conv)\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, kernel_size, padding=0):\n",
    "        super().__init__()\n",
    "        self.depthwise = nn.Conv1d(\n",
    "            in_channels, in_channels, \n",
    "            kernel_size=kernel_size,\n",
    "            padding=padding,\n",
    "            groups=in_channels\n",
    "        )\n",
    "        self.pointwise = nn.Conv1d(in_channels, out_channels, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.depthwise(x)\n",
    "        x = self.pointwise(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MultiKernelConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-kernel convolution block with proper channel handling.\n",
    "    Fixed: Handles remainder when dividing channels into thirds.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, seq_len, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Split channels properly, handling remainder\n",
    "        c1 = out_channels // 3\n",
    "        c2 = out_channels // 3\n",
    "        c3 = out_channels - c1 - c2  # Gets the remainder\n",
    "        \n",
    "        self.conv1 = DepthwiseSeparableConv1d(in_channels, c1, kernel_size=1)\n",
    "        self.conv3 = DepthwiseSeparableConv1d(in_channels, c2, kernel_size=3, padding=1)\n",
    "        self.conv5 = DepthwiseSeparableConv1d(in_channels, c3, kernel_size=5, padding=2)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm([c1, seq_len])\n",
    "        self.norm2 = nn.LayerNorm([c2, seq_len])\n",
    "        self.norm3 = nn.LayerNorm([c3, seq_len])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Apply different kernel sizes in parallel\n",
    "        x1 = F.gelu(self.norm1(self.conv1(x)))\n",
    "        x2 = F.gelu(self.norm2(self.conv3(x)))\n",
    "        x3 = F.gelu(self.norm3(self.conv5(x)))\n",
    "        \n",
    "        # Concatenate multi-kernel outputs (now sums to exactly out_channels)\n",
    "        out = torch.cat([x1, x2, x3], dim=1)\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class AlignmentModule(nn.Module):\n",
    "    \"\"\"\n",
    "    Alignment module inspired by DNAFormer.\n",
    "    Processes each read individually to learn alignment before NCI.\n",
    "    Lighter than DNAFormer (uses 2 conv blocks instead of more complex architecture).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim, out_channels, seq_len, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.conv_block1 = MultiKernelConvBlock(embed_dim, out_channels, seq_len, dropout)\n",
    "        self.conv_block2 = MultiKernelConvBlock(out_channels, out_channels, seq_len, dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, cluster_size, embed_dim, seq_len)\n",
    "        batch, cluster, emb, seq = x.shape\n",
    "        \n",
    "        # Process each read independently\n",
    "        x = x.view(batch * cluster, emb, seq)\n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "        \n",
    "        # Reshape back\n",
    "        x = x.view(batch, cluster, -1, seq)\n",
    "        return x\n",
    "\n",
    "\n",
    "class EmbeddingModule(nn.Module):\n",
    "    \"\"\"\n",
    "    Embedding module - processes cluster after NCI to extract cluster-level features.\n",
    "    Fixed: Linear layer now correctly transforms sequence length.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, in_len, out_len, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.conv_block = MultiKernelConvBlock(in_channels, out_channels, in_len, dropout)\n",
    "        \n",
    "        # Linear projection to target length\n",
    "        self.linear = nn.Linear(in_len, out_len)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, channels, seq_len)\n",
    "        x = self.conv_block(x)  # (B, out_channels, in_len)\n",
    "        \n",
    "        # Apply linear transformation to sequence dimension\n",
    "        # Reshape to apply linear independently to each channel\n",
    "        batch, channels, seq_len = x.shape\n",
    "        \n",
    "        # Reshape: (B, C, L) -> (B*C, L)\n",
    "        x = x.reshape(batch * channels, seq_len)\n",
    "        \n",
    "        # Apply linear: (B*C, in_len) -> (B*C, out_len)\n",
    "        x = self.linear(x)\n",
    "        \n",
    "        # Reshape back: (B*C, out_len) -> (B, C, out_len)\n",
    "        x = x.reshape(batch, channels, -1)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# IMPROVED MODEL (DNAFormer-inspired BiGRU)\n",
    "# =============================================================================\n",
    "\n",
    "class ImprovedDNAReconstructionModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Improved DNA reconstruction model inspired by DNAFormer architecture.\n",
    "    \n",
    "    Architecture:\n",
    "    1. Embedding layer\n",
    "    2. Alignment module (per-read processing with multi-kernel convs)\n",
    "    3. NCI (Non-Coherent Integration) - sum over cluster dimension\n",
    "    4. Embedding module (cluster-level feature extraction)\n",
    "    5. BiGRU (instead of Transformer for efficiency)\n",
    "    6. Output projection\n",
    "    \n",
    "    Key differences from your original BiGRU:\n",
    "    - Adds per-read alignment processing BEFORE averaging\n",
    "    - Uses multi-kernel convolutions to capture indel patterns\n",
    "    - Processes cluster as a whole after NCI\n",
    "    - Still uses BiGRU (not Transformer) to keep params low\n",
    "    \n",
    "    Parameters: ~5-8M (vs DNAFormer's 100M, your BiGRU's ~2M)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, label_seq_len, max_read_len, padding_idx,\n",
    "                 embed_dim=128, alignment_filters=128, embedding_filters=256,\n",
    "                 gru_hidden=256, gru_layers=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.label_seq_len = label_seq_len\n",
    "        self.max_read_len = max_read_len\n",
    "        \n",
    "        # 1. Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=padding_idx)\n",
    "        \n",
    "        # 2. Alignment module (per-read processing)\n",
    "        self.alignment = AlignmentModule(\n",
    "            embed_dim=embed_dim,\n",
    "            out_channels=alignment_filters,\n",
    "            seq_len=max_read_len,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # 3. NCI (Non-Coherent Integration) is just a sum - no learnable params\n",
    "        \n",
    "        # 4. Embedding module (cluster-level processing)\n",
    "        self.embedding_module = EmbeddingModule(\n",
    "            in_channels=alignment_filters,\n",
    "            out_channels=embedding_filters,\n",
    "            in_len=max_read_len,\n",
    "            out_len=label_seq_len,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # 5. BiGRU for sequence modeling\n",
    "        self.gru = nn.GRU(\n",
    "            embedding_filters,\n",
    "            gru_hidden,\n",
    "            num_layers=gru_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout if gru_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # 6. Output projection\n",
    "        self.fc_out = nn.Linear(gru_hidden * 2, vocab_size)\n",
    "    \n",
    "    def forward(self, cluster_batch):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            cluster_batch: (batch_size, max_cluster_size, max_read_len)\n",
    "        \n",
    "        Returns:\n",
    "            logits: (batch_size, label_seq_len, vocab_size)\n",
    "        \"\"\"\n",
    "        # 1. Embed all reads\n",
    "        embedded = self.embedding(cluster_batch)  # (B, cluster, read_len, embed_dim)\n",
    "        embedded = embedded.permute(0, 1, 3, 2)  # (B, cluster, embed_dim, read_len)\n",
    "        \n",
    "        # 2. Alignment module - process each read independently\n",
    "        aligned = self.alignment(embedded)  # (B, cluster, alignment_filters, read_len)\n",
    "        \n",
    "        # 3. NCI (Non-Coherent Integration) - sum over cluster dimension\n",
    "        # This is DNAFormer's key insight: improves SNR and robustness to cluster size\n",
    "        nci_output = torch.sum(aligned, dim=1)  # (B, alignment_filters, read_len)\n",
    "        \n",
    "        # 4. Embedding module - process cluster as a whole\n",
    "        cluster_features = self.embedding_module(nci_output)  # (B, embedding_filters, label_seq_len)\n",
    "        \n",
    "        # 5. Prepare for GRU: (B, seq_len, features)\n",
    "        x = cluster_features.permute(0, 2, 1)  # (B, label_seq_len, embedding_filters)\n",
    "        \n",
    "        # 6. BiGRU\n",
    "        gru_out, _ = self.gru(x)  # (B, label_seq_len, gru_hidden*2)\n",
    "        gru_out = self.dropout(gru_out)\n",
    "        \n",
    "        # 7. Output projection\n",
    "        logits = self.fc_out(gru_out)  # (B, label_seq_len, vocab_size)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# COSINE ANNEALING WARMUP SCHEDULER\n",
    "# =============================================================================\n",
    "\n",
    "class CosineAnnealingWarmupScheduler:\n",
    "    \"\"\"Cosine annealing learning rate scheduler with warmup (DNAFormer uses this)\"\"\"\n",
    "    \n",
    "    def __init__(self, optimizer, warmup_epochs, total_epochs, max_lr, min_lr):\n",
    "        self.optimizer = optimizer\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.total_epochs = total_epochs\n",
    "        self.max_lr = max_lr\n",
    "        self.min_lr = min_lr\n",
    "        self.current_epoch = 0\n",
    "    \n",
    "    def step(self):\n",
    "        if self.current_epoch < self.warmup_epochs:\n",
    "            # Linear warmup\n",
    "            lr = self.max_lr * (self.current_epoch + 1) / self.warmup_epochs\n",
    "        else:\n",
    "            # Cosine annealing\n",
    "            progress = (self.current_epoch - self.warmup_epochs) / (self.total_epochs - self.warmup_epochs)\n",
    "            lr = self.min_lr + (self.max_lr - self.min_lr) * 0.5 * (1 + math.cos(math.pi * progress))\n",
    "        \n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        \n",
    "        self.current_epoch += 1\n",
    "        return lr\n",
    "    \n",
    "    def get_last_lr(self):\n",
    "        return [group['lr'] for group in self.optimizer.param_groups]\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# TRAINING UTILITIES\n",
    "# =============================================================================\n",
    "\n",
    "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    loop = tqdm(loader, desc=\"Training\", leave=False)\n",
    "    \n",
    "    for clusters, labels in loop:\n",
    "        clusters, labels = clusters.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(clusters)\n",
    "        loss = criterion(logits.view(-1, VOCAB_SIZE), labels.view(-1))\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "    \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    \"\"\"Validate the model\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    loop = tqdm(loader, desc=\"Validating\", leave=False)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for clusters, labels in loop:\n",
    "            clusters, labels = clusters.to(device), labels.to(device)\n",
    "            logits = model(clusters)\n",
    "            loss = criterion(logits.view(-1, VOCAB_SIZE), labels.view(-1))\n",
    "            total_loss += loss.item()\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "    \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN EXECUTION\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # =========================================================================\n",
    "    # LOAD DATA\n",
    "    # =========================================================================\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"LOADING DATA\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    full_train_dataset = DnaClusterDataset(\n",
    "        filepath=TRAIN_FILE,\n",
    "        max_cluster_size=MAX_CLUSTER_SIZE,\n",
    "        max_read_len=MAX_READ_LEN,\n",
    "        label_seq_len=LABEL_SEQ_LEN,\n",
    "        char_to_int=VOCAB,\n",
    "        padding_idx=PADDING_IDX\n",
    "    )\n",
    "    \n",
    "    # 98% train, 2% validation\n",
    "    val_size = int(0.02 * len(full_train_dataset))\n",
    "    if val_size < BATCH_SIZE:\n",
    "        val_size = min(BATCH_SIZE * 2, len(full_train_dataset) // 10)\n",
    "    train_size = len(full_train_dataset) - val_size\n",
    "    \n",
    "    train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
    "    \n",
    "    print(f\"\\nTotal clusters: {len(full_train_dataset):,}\")\n",
    "    print(f\"Training:       {len(train_dataset):,}\")\n",
    "    print(f\"Validation:     {len(val_dataset):,}\")\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "        num_workers=2, pin_memory=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "        num_workers=2, pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # =========================================================================\n",
    "    # INITIALIZE MODEL\n",
    "    # =========================================================================\n",
    "    \n",
    "    model = ImprovedDNAReconstructionModel(\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        label_seq_len=LABEL_SEQ_LEN,\n",
    "        max_read_len=MAX_READ_LEN,\n",
    "        padding_idx=PADDING_IDX,\n",
    "        embed_dim=EMBED_DIM,\n",
    "        alignment_filters=ALIGNMENT_FILTERS,\n",
    "        embedding_filters=EMBEDDING_FILTERS,\n",
    "        gru_hidden=GRU_HIDDEN,\n",
    "        gru_layers=GRU_LAYERS,\n",
    "        dropout=DROPOUT\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    num_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"MODEL ARCHITECTURE\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Total parameters: {num_params:,} ({num_params/1e6:.2f}M)\")\n",
    "    print(f\"Embedding dim:    {EMBED_DIM}\")\n",
    "    print(f\"Alignment:        {ALIGNMENT_FILTERS} filters\")\n",
    "    print(f\"Embedding:        {EMBEDDING_FILTERS} filters\")\n",
    "    print(f\"GRU hidden:       {GRU_HIDDEN}\")\n",
    "    print(f\"GRU layers:       {GRU_LAYERS}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=PADDING_IDX, label_smoothing=0.1)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    \n",
    "    scheduler = CosineAnnealingWarmupScheduler(\n",
    "        optimizer=optimizer,\n",
    "        warmup_epochs=WARMUP_EPOCHS,\n",
    "        total_epochs=EPOCHS,\n",
    "        max_lr=LEARNING_RATE,\n",
    "        min_lr=MIN_LR\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nOptimizer: Adam\")\n",
    "    print(f\"LR Schedule: Warmup {WARMUP_EPOCHS} â†’ Cosine {LEARNING_RATE:.2e} â†’ {MIN_LR:.2e}\")\n",
    "    print(f\"Batch size: {BATCH_SIZE}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # TRAINING LOOP\n",
    "    # =========================================================================\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"TRAINING: {DATASET_NAME}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    learning_rates = []\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        start_time = time.time()\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        \n",
    "        # Train and validate\n",
    "        train_loss = train_one_epoch(model, train_loader, criterion, optimizer, DEVICE)\n",
    "        val_loss = validate(model, val_loader, criterion, DEVICE)\n",
    "        \n",
    "        # Step scheduler\n",
    "        new_lr = scheduler.step()\n",
    "        \n",
    "        # Record history\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        learning_rates.append(current_lr)\n",
    "        \n",
    "        # Print progress\n",
    "        print(f\"Epoch {epoch+1:3d}/{EPOCHS} | \"\n",
    "              f\"LR: {current_lr:.2e} | \"\n",
    "              f\"Train: {train_loss:.4f} | \"\n",
    "              f\"Val: {val_loss:.4f} | \"\n",
    "              f\"Time: {time.time()-start_time:.1f}s\", end=\"\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            improvement = best_val_loss - val_loss\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), WEIGHTS_DIR / f\"best_model_{DATASET_NAME}.pth\")\n",
    "            print(f\" âœ“ BEST (â†“{improvement:.4f})\")\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            print()\n",
    "            patience_counter += 1\n",
    "        \n",
    "        # Early stopping\n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    # Save final model\n",
    "    torch.save(model.state_dict(), WEIGHTS_DIR / f\"final_model_{DATASET_NAME}.pth\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Training Finished\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Best val loss: {best_val_loss:.4f}\")\n",
    "    print(f\"Models saved to: {WEIGHTS_DIR}\")\n",
    "    \n",
    "    # Save training history\n",
    "    pd.DataFrame({\n",
    "        'epoch': range(1, len(train_losses)+1),\n",
    "        'train_loss': train_losses,\n",
    "        'val_loss': val_losses,\n",
    "        'learning_rate': learning_rates\n",
    "    }).to_csv(RESULTS_DIR / f\"history_{DATASET_NAME}.csv\", index=False)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # EVALUATION\n",
    "    # =========================================================================\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"EVALUATION: {DATASET_NAME}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Load best model\n",
    "    inference_model = ImprovedDNAReconstructionModel(\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        label_seq_len=LABEL_SEQ_LEN,\n",
    "        max_read_len=MAX_READ_LEN,\n",
    "        padding_idx=PADDING_IDX,\n",
    "        embed_dim=EMBED_DIM,\n",
    "        alignment_filters=ALIGNMENT_FILTERS,\n",
    "        embedding_filters=EMBEDDING_FILTERS,\n",
    "        gru_hidden=GRU_HIDDEN,\n",
    "        gru_layers=GRU_LAYERS,\n",
    "        dropout=DROPOUT\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    model_path = WEIGHTS_DIR / f\"best_model_{DATASET_NAME}.pth\"\n",
    "    inference_model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
    "    inference_model.eval()\n",
    "    print(f\"âœ“ Loaded model from {model_path}\")\n",
    "    \n",
    "    # Load test data\n",
    "    test_dataset = DnaClusterDataset(\n",
    "        filepath=EVAL_FILE,\n",
    "        max_cluster_size=MAX_CLUSTER_SIZE,\n",
    "        max_read_len=MAX_READ_LEN,\n",
    "        label_seq_len=LABEL_SEQ_LEN,\n",
    "        char_to_int=VOCAB,\n",
    "        padding_idx=PADDING_IDX\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=BATCH_SIZE * 2, shuffle=False,\n",
    "        num_workers=2, pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    total_clusters = 0\n",
    "    failed_clusters = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for clusters, labels in tqdm(test_loader, desc=\"Testing\"):\n",
    "            clusters = clusters.to(DEVICE)\n",
    "            logits = inference_model(clusters)\n",
    "            predictions = torch.argmax(logits, dim=2).cpu()\n",
    "            \n",
    "            for i in range(labels.shape[0]):\n",
    "                total_clusters += 1\n",
    "                pred_seq = decode_seq(predictions[i], INT_TO_CHAR)\n",
    "                label_seq = decode_seq(labels[i], INT_TO_CHAR)\n",
    "                if pred_seq != label_seq:\n",
    "                    failed_clusters += 1\n",
    "    \n",
    "    failure_rate = (failed_clusters / total_clusters) * 100 if total_clusters > 0 else 0\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"RESULTS: {DATASET_NAME}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Total clusters:  {total_clusters:,}\")\n",
    "    print(f\"Failed clusters: {failed_clusters}\")\n",
    "    print(f\"Failure rate:    {failure_rate:.4f}%\")\n",
    "    print(f\"Target:          {CONFIG['target_failure']}%\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    if failure_rate <= CONFIG['target_failure']:\n",
    "        print(f\"\\nðŸŽ‰ SUCCESS! Target achieved!\")\n",
    "    else:\n",
    "        gap = failure_rate - CONFIG['target_failure']\n",
    "        print(f\"\\nðŸ“ˆ Gap to target: {gap:.4f}%\")\n",
    "    \n",
    "    # Save results\n",
    "    pd.DataFrame([{\n",
    "        'dataset': DATASET_NAME,\n",
    "        'total_clusters': total_clusters,\n",
    "        'failed_clusters': failed_clusters,\n",
    "        'failure_rate_percent': failure_rate,\n",
    "        'target_percent': CONFIG['target_failure'],\n",
    "        'model_params': num_params\n",
    "    }]).to_csv(RESULTS_DIR / f\"results_{DATASET_NAME}.csv\", index=False)\n",
    "    \n",
    "    print(f\"\\nResults saved to: {RESULTS_DIR}\")\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"COMPLETE!\")\n",
    "    print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f75b94-1217-454b-97ef-9fe935b6a2eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
