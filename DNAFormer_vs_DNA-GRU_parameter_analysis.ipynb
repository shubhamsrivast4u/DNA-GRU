{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Count and FLOPs Analysis: DNAFormer vs Compact BiGRU Model\n",
    "\n",
    "This notebook provides a comprehensive module-by-module analysis of:\n",
    "1. **DNAFormer** (exact architecture from the paper and GitHub code)\n",
    "2. **Compact BiGRU Model** (your implementation)\n",
    "\n",
    "For datasets: Srinivasavaradhan, Grass, Erlich, BinnedNanoporeTwoFlowcells, BinnedTestIllumina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from typing import Dict, Tuple, List\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Configurations\n",
    "\n",
    "Each dataset has different sequence lengths which affect parameter counts and FLOPs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Configurations:\n",
      "================================================================================\n",
      "Srinivasavaradhan:\n",
      "  Label length: 110\n",
      "  Max deviation: 10\n",
      "  Index length: 0\n",
      "\n",
      "Grass:\n",
      "  Label length: 117\n",
      "  Max deviation: 11\n",
      "  Index length: 0\n",
      "\n",
      "Erlich:\n",
      "  Label length: 152\n",
      "  Max deviation: 10\n",
      "  Index length: 0\n",
      "\n",
      "BinnedNanoporeTwoFlowcells:\n",
      "  Label length: 128\n",
      "  Max deviation: 4\n",
      "  Index length: 12\n",
      "\n",
      "BinnedTestIllumina:\n",
      "  Label length: 128\n",
      "  Max deviation: 4\n",
      "  Index length: 12\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dataset configurations from DNAFormer paper and your code\n",
    "DATASET_CONFIGS = {\n",
    "    \"Srinivasavaradhan\": {\n",
    "        \"label_len\": 110,\n",
    "        \"max_deviation\": 10,\n",
    "        \"index_length\": 0,  # No index filtering for public datasets\n",
    "        \"description\": \"Srinivasavaradhan et al. - Twist, MinION (high error)\",\n",
    "    },\n",
    "    \"Grass\": {\n",
    "        \"label_len\": 117,\n",
    "        \"max_deviation\": 11,\n",
    "        \"index_length\": 0,\n",
    "        \"description\": \"Grass et al. (2015) - CustomArray, Illumina miSeq\",\n",
    "    },\n",
    "    \"Erlich\": {\n",
    "        \"label_len\": 152,\n",
    "        \"max_deviation\": 10,\n",
    "        \"index_length\": 0,\n",
    "        \"description\": \"Erlich et al. (2017) - DNA Fountain, Illumina miSeq\",\n",
    "    },\n",
    "    \"BinnedNanoporeTwoFlowcells\": {\n",
    "        \"label_len\": 128,\n",
    "        \"max_deviation\": 4,\n",
    "        \"index_length\": 12,  # DNAFormer filters index\n",
    "        \"description\": \"DNAformer Nanopore Two Flowcells Combined\",\n",
    "    },\n",
    "    \"BinnedTestIllumina\": {\n",
    "        \"label_len\": 128,\n",
    "        \"max_deviation\": 4,\n",
    "        \"index_length\": 12,  # DNAFormer filters index\n",
    "        \"description\": \"DNAformer Test Illumina - Twist + miSeq\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# Print dataset info\n",
    "print(\"Dataset Configurations:\")\n",
    "print(\"=\" * 80)\n",
    "for name, cfg in DATASET_CONFIGS.items():\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Label length: {cfg['label_len']}\")\n",
    "    print(f\"  Max deviation: {cfg['max_deviation']}\")\n",
    "    print(f\"  Index length: {cfg['index_length']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PART 1: DNAFormer Architecture (Exact from Paper/Code)\n",
    "\n",
    "## DNAFormer Hyperparameters (from supplementary material):\n",
    "- `n_head = 32`\n",
    "- `num_layers = 12`\n",
    "- `d_model = 1024`\n",
    "- `alignment_filters = 128`\n",
    "- `dim_feedforward = 2048`\n",
    "- `output_ch = 4`\n",
    "- `enc_filters = 4` (one-hot DNA encoding)\n",
    "- Kernel sizes: {1, 3, 5, 7}\n",
    "- Siamese architecture (2 branches, shared weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# DNAFormer: Depthwise Separable Conv1D (EXACT FROM DNAFormer CODE)\n",
    "# ==============================================================================\n",
    "\n",
    "class depthwise_separable_conv_1d(nn.Module):\n",
    "    \"\"\"\n",
    "    Exact implementation from DNAFormer GitHub code.\n",
    "    \n",
    "    Parameters:\n",
    "    - depthwise: in_ch * kernels_per_layer * kernel_size (weights) + in_ch * kernels_per_layer (bias)\n",
    "    - pointwise: in_ch * kernels_per_layer * out_ch (weights) + out_ch (bias)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, out_ch, kernels_per_layer=1, kernel_size=3, stride=1, padding=0):\n",
    "        super(depthwise_separable_conv_1d, self).__init__()\n",
    "        self.depthwise = nn.Conv1d(in_ch, in_ch * kernels_per_layer, kernel_size=kernel_size, \n",
    "                                   stride=stride, padding=padding, groups=in_ch)\n",
    "        self.pointwise = nn.Conv1d(in_ch * kernels_per_layer, out_ch, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.depthwise(x)\n",
    "        out = self.pointwise(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# DNAFormer: Double Conv1D Block (EXACT FROM DNAFormer CODE)\n",
    "# ==============================================================================\n",
    "\n",
    "class double_conv1D(nn.Module):\n",
    "    \"\"\"\n",
    "    (conv => norm => act) * 2\n",
    "    Exact implementation from DNAFormer GitHub code.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, out_ch, seq_len, padding=0, kernel_size=3, stride=1, p_dropout=0):\n",
    "        super(double_conv1D, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            depthwise_separable_conv_1d(in_ch, out_ch, kernels_per_layer=1, \n",
    "                                        kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            nn.LayerNorm(seq_len, elementwise_affine=True),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p_dropout),\n",
    "            depthwise_separable_conv_1d(out_ch, out_ch, kernels_per_layer=1, \n",
    "                                        kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            nn.LayerNorm(seq_len, elementwise_affine=True),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p_dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# DNAFormer: Linear Block (EXACT FROM DNAFormer CODE)\n",
    "# ==============================================================================\n",
    "\n",
    "class linear_block(nn.Module):\n",
    "    \"\"\"\n",
    "    Linear block for sequence length transformation.\n",
    "    Exact implementation from DNAFormer GitHub code.\n",
    "    \n",
    "    Structure: Linear -> LayerNorm -> GELU -> Dropout -> Linear -> LayerNorm -> GELU -> Dropout -> Linear\n",
    "    \"\"\"\n",
    "    def __init__(self, input_len, output_len, p_dropout=0):\n",
    "        super(linear_block, self).__init__()\n",
    "        self.fc_1 = nn.Linear(input_len, output_len)\n",
    "        self.norm_1 = nn.LayerNorm(output_len, elementwise_affine=True)\n",
    "        self.act_1 = nn.GELU()\n",
    "        self.dout_1 = nn.Dropout(p_dropout)\n",
    "        self.fc_2 = nn.Linear(output_len, output_len)\n",
    "        self.norm_2 = nn.LayerNorm(output_len, elementwise_affine=True)\n",
    "        self.act_2 = nn.GELU()\n",
    "        self.dout_2 = nn.Dropout(p_dropout)\n",
    "        self.fc_3 = nn.Linear(output_len, output_len)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc_1(x)\n",
    "        x = self.norm_1(x)\n",
    "        x = self.act_1(x)\n",
    "        x = self.dout_1(x)\n",
    "        x = self.act_2(self.norm_2(self.fc_2(x)))\n",
    "        x = self.dout_2(x)\n",
    "        x = self.fc_3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# DNAFormer: Alignment Module (EXACT FROM DNAFormer CODE)\n",
    "# ==============================================================================\n",
    "\n",
    "class alignement_module(nn.Module):\n",
    "    \"\"\"\n",
    "    DNAFormer Alignment Module.\n",
    "    Exact implementation from DNAFormer GitHub code.\n",
    "    \n",
    "    - 4 parallel double_conv1D blocks with kernel sizes {1, 3, 5, 7}\n",
    "    - Each outputs alignment_filters // 4 channels (32 channels each)\n",
    "    - Concatenate -> Linear block\n",
    "    - Processes each read INDEPENDENTLY\n",
    "    \"\"\"\n",
    "    def __init__(self, enc_filters, alignment_filters, noisy_copies_length, p_dropout=0):\n",
    "        super(alignement_module, self).__init__()\n",
    "        \n",
    "        out_ch = alignment_filters // 4  # 32 channels per branch\n",
    "        \n",
    "        self.conv_block_1 = double_conv1D(enc_filters, out_ch, seq_len=noisy_copies_length, \n",
    "                                          kernel_size=1, p_dropout=p_dropout)\n",
    "        self.conv_block_2 = double_conv1D(enc_filters, out_ch, seq_len=noisy_copies_length, \n",
    "                                          kernel_size=3, padding=1, p_dropout=p_dropout)\n",
    "        self.conv_block_3 = double_conv1D(enc_filters, out_ch, seq_len=noisy_copies_length, \n",
    "                                          kernel_size=5, padding=2, p_dropout=p_dropout)\n",
    "        self.conv_block_4 = double_conv1D(enc_filters, out_ch, seq_len=noisy_copies_length, \n",
    "                                          kernel_size=7, padding=3, p_dropout=p_dropout)\n",
    "        \n",
    "        self.linear_block = linear_block(input_len=noisy_copies_length, \n",
    "                                         output_len=noisy_copies_length, \n",
    "                                         p_dropout=p_dropout)\n",
    "   \n",
    "    def forward(self, x):      \n",
    "        batch, cluster, emb, seq = x.shape\n",
    "        \n",
    "        # Reshape: (batch, cluster, emb, seq) -> (batch*cluster, emb, seq)\n",
    "        x = x.view(batch * cluster, emb, seq)\n",
    "        \n",
    "        # Apply 4 kernel branches in parallel\n",
    "        x = torch.cat([self.conv_block_1(x), self.conv_block_2(x), \n",
    "                       self.conv_block_3(x), self.conv_block_4(x)], dim=1)\n",
    "        x = self.linear_block(x)\n",
    "        \n",
    "        # Reshape back\n",
    "        x = x.view(batch, cluster, -1, seq)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# DNAFormer: Embedding Module (EXACT FROM DNAFormer CODE)\n",
    "# ==============================================================================\n",
    "\n",
    "class embedding_module(nn.Module):\n",
    "    \"\"\"\n",
    "    DNAFormer Embedding Module (after NCI).\n",
    "    Exact implementation from DNAFormer GitHub code.\n",
    "    \n",
    "    - Same structure as alignment module\n",
    "    - Input: alignment_filters channels (after NCI sum)\n",
    "    - Output: d_model channels, with seq_len -> label_length\n",
    "    \"\"\"\n",
    "    def __init__(self, alignment_filters, d_model, noisy_copies_length, label_length, p_dropout=0):\n",
    "        super(embedding_module, self).__init__()\n",
    "        \n",
    "        self.label_length = label_length\n",
    "        out_ch = d_model // 4  # 256 channels per branch\n",
    "        \n",
    "        self.conv_block_1 = double_conv1D(alignment_filters, out_ch, seq_len=noisy_copies_length, \n",
    "                                          kernel_size=1, p_dropout=p_dropout)\n",
    "        self.conv_block_2 = double_conv1D(alignment_filters, out_ch, seq_len=noisy_copies_length, \n",
    "                                          kernel_size=3, padding=1, p_dropout=p_dropout)\n",
    "        self.conv_block_3 = double_conv1D(alignment_filters, out_ch, seq_len=noisy_copies_length, \n",
    "                                          kernel_size=5, padding=2, p_dropout=p_dropout)\n",
    "        self.conv_block_4 = double_conv1D(alignment_filters, out_ch, seq_len=noisy_copies_length, \n",
    "                                          kernel_size=7, padding=3, p_dropout=p_dropout)\n",
    "        \n",
    "        # Linear projects from noisy_copies_length to label_length\n",
    "        self.linear_block = linear_block(input_len=noisy_copies_length, \n",
    "                                         output_len=label_length, \n",
    "                                         p_dropout=p_dropout)\n",
    "                   \n",
    "    def forward(self, x):\n",
    "        # Sum over cluster dimension (Non-Coherent Integration)\n",
    "        x = torch.sum(x, dim=1)\n",
    "        \n",
    "        # Feature extraction with 4 kernel branches\n",
    "        x = torch.cat([self.conv_block_1(x), self.conv_block_2(x), \n",
    "                       self.conv_block_3(x), self.conv_block_4(x)], dim=1)\n",
    "        x = self.linear_block(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# DNAFormer: Output Module (EXACT FROM DNAFormer CODE)\n",
    "# ==============================================================================\n",
    "\n",
    "class output_module(nn.Module):\n",
    "    \"\"\"\n",
    "    DNAFormer Output Module.\n",
    "    Exact implementation from DNAFormer GitHub code.\n",
    "    \n",
    "    3 Conv1x1 layers: d_model -> d_model -> d_model -> output_ch (4)\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, output_ch):\n",
    "        super(output_module, self).__init__()\n",
    "        \n",
    "        self.conv_1 = nn.Conv1d(d_model, d_model, 1)\n",
    "        self.conv_2 = nn.Conv1d(d_model, d_model, 1)\n",
    "        self.conv_3 = nn.Conv1d(d_model, output_ch, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_1(x)\n",
    "        x = self.conv_2(x)\n",
    "        x = self.conv_3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# DNAFormer: Fusion Module (EXACT FROM DNAFormer CODE)\n",
    "# ==============================================================================\n",
    "\n",
    "class fusion_module(nn.Module):\n",
    "    \"\"\"\n",
    "    DNAFormer Fusion Module.\n",
    "    Exact implementation from DNAFormer GitHub code.\n",
    "    \n",
    "    - 2 learnable fusion vectors (left, right) of length label_length\n",
    "    - 3 Conv1x1 layers for refinement\n",
    "    \"\"\"\n",
    "    def __init__(self, output_ch, label_length):\n",
    "        super(fusion_module, self).__init__()\n",
    "        \n",
    "        self.pred_fusion_left = nn.Parameter(torch.ones(label_length))\n",
    "        self.pred_fusion_right = nn.Parameter(torch.ones(label_length))\n",
    "        \n",
    "        self.conv_1 = nn.Conv1d(output_ch, output_ch, 1)\n",
    "        self.conv_2 = nn.Conv1d(output_ch, output_ch, 1)\n",
    "        self.conv_3 = nn.Conv1d(output_ch, output_ch, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_left = x[:x.shape[0]//2, :, :]\n",
    "        x_right = torch.flip(x[x.shape[0]//2:, :, :], dims=[-1])\n",
    "        x = (x_left * self.pred_fusion_left + x_right * self.pred_fusion_right) / 2\n",
    "        \n",
    "        x = self.conv_1(x)\n",
    "        x = self.conv_2(x)\n",
    "        x = self.conv_3(x)\n",
    "        \n",
    "        return x, x_left, x_right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# DNAFormer: Complete Model (EXACT FROM DNAFormer CODE)\n",
    "# ==============================================================================\n",
    "\n",
    "class DNAFormer(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete DNAFormer model (Siamese architecture with shared weights).\n",
    "    Exact implementation from DNAFormer GitHub code.\n",
    "    \n",
    "    Architecture:\n",
    "    1. Alignment Module (per-read, 4 kernel multi-head conv + linear)\n",
    "    2. NCI (sum over cluster dimension) - no parameters, inside embedding_module\n",
    "    3. Embedding Module (cluster-level, 4 kernel multi-head conv + linear)\n",
    "    4. Transformer Encoder (12 layers, 32 heads, d_model=1024, ff=2048)\n",
    "    5. Output Module (3 Conv1x1)\n",
    "    6. Fusion Module (learnable vectors + 3 Conv1x1)\n",
    "    \n",
    "    DNAFormer Hyperparameters (from paper):\n",
    "    - n_head = 32\n",
    "    - num_layers = 12\n",
    "    - d_model = 1024\n",
    "    - alignment_filters = 128\n",
    "    - dim_feedforward = 2048\n",
    "    - output_ch = 4\n",
    "    - enc_filters = 4\n",
    "    \"\"\"\n",
    "    def __init__(self, enc_filters, alignment_filters, d_model, n_head, \n",
    "                 num_layers, dim_feedforward, output_ch, noisy_copies_length, \n",
    "                 label_length, p_dropout=0):\n",
    "        super(DNAFormer, self).__init__()\n",
    "        \n",
    "        # Store config\n",
    "        self.noisy_copies_length = noisy_copies_length\n",
    "        self.label_length = label_length\n",
    "        \n",
    "        # 1. Alignment Module\n",
    "        self.alignement = alignement_module(\n",
    "            enc_filters, alignment_filters, noisy_copies_length, p_dropout\n",
    "        )\n",
    "        \n",
    "        # 2. Embedding Module (includes NCI)\n",
    "        self.embedding = embedding_module(\n",
    "            alignment_filters, d_model, noisy_copies_length, label_length, p_dropout\n",
    "        )\n",
    "        \n",
    "        # 3. Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_head,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=p_dropout,\n",
    "            activation='gelu',\n",
    "            batch_first=False\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # 4. Output Module\n",
    "        self.output_module = output_module(d_model, output_ch)\n",
    "        \n",
    "        # 5. Fusion Module\n",
    "        self.fusion = fusion_module(output_ch, label_length)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Alignment module (per-read processing)\n",
    "        x = self.alignement(x)\n",
    "        \n",
    "        # Embedding module (includes NCI sum)\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # Transformer: (batch, d_model, seq) -> (seq, batch, d_model)\n",
    "        x = x.permute(2, 0, 1)\n",
    "        x = self.encoder(x)\n",
    "        x = x.permute(1, 2, 0)  # Back to (batch, d_model, seq)\n",
    "        \n",
    "        # Output module\n",
    "        x = self.output_module(x)\n",
    "        \n",
    "        # Fusion module\n",
    "        x, x_left, x_right = self.fusion(x)\n",
    "        \n",
    "        return {'pred': x, 'pred_left': x_left, 'pred_right': x_right}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PART 2: Your Compact BiGRU Model (Exact from working-BinnedNanoporeTwoFlowcells.py)\n",
    "\n",
    "## Compact Model Hyperparameters (from your code):\n",
    "- `embed_dim = 300`\n",
    "- `alignment_filters = 128`\n",
    "- `embedding_filters = 500`\n",
    "- `gru_hidden = 300`\n",
    "- `gru_layers = 2`\n",
    "- Kernel sizes: {1, 3, 5} (only 3 kernels vs DNAFormer's 4)\n",
    "- **No Siamese architecture** (single branch)\n",
    "- **No Fusion module**\n",
    "- **BiGRU instead of Transformer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Compact Model: Depthwise Separable Conv1d (FROM YOUR CODE)\n",
    "# ==============================================================================\n",
    "\n",
    "class DepthwiseSeparableConv1d(nn.Module):\n",
    "    \"\"\"Depthwise separable convolution (more efficient than standard conv)\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, kernel_size, padding=0):\n",
    "        super().__init__()\n",
    "        self.depthwise = nn.Conv1d(\n",
    "            in_channels, in_channels, \n",
    "            kernel_size=kernel_size,\n",
    "            padding=padding,\n",
    "            groups=in_channels\n",
    "        )\n",
    "        self.pointwise = nn.Conv1d(in_channels, out_channels, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.depthwise(x)\n",
    "        x = self.pointwise(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Compact Model: Multi-Kernel Conv Block (FROM YOUR CODE)\n",
    "# ==============================================================================\n",
    "\n",
    "class MultiKernelConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-kernel convolution block with proper channel handling.\n",
    "    Uses 3 kernel sizes {1, 3, 5} instead of DNAFormer's 4 {1, 3, 5, 7}.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, seq_len, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Split channels properly, handling remainder\n",
    "        c1 = out_channels // 3\n",
    "        c2 = out_channels // 3\n",
    "        c3 = out_channels - c1 - c2  # Gets the remainder\n",
    "        \n",
    "        self.conv1 = DepthwiseSeparableConv1d(in_channels, c1, kernel_size=1)\n",
    "        self.conv3 = DepthwiseSeparableConv1d(in_channels, c2, kernel_size=3, padding=1)\n",
    "        self.conv5 = DepthwiseSeparableConv1d(in_channels, c3, kernel_size=5, padding=2)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm([c1, seq_len])\n",
    "        self.norm2 = nn.LayerNorm([c2, seq_len])\n",
    "        self.norm3 = nn.LayerNorm([c3, seq_len])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Apply different kernel sizes in parallel\n",
    "        x1 = F.gelu(self.norm1(self.conv1(x)))\n",
    "        x2 = F.gelu(self.norm2(self.conv3(x)))\n",
    "        x3 = F.gelu(self.norm3(self.conv5(x)))\n",
    "        \n",
    "        # Concatenate multi-kernel outputs\n",
    "        out = torch.cat([x1, x2, x3], dim=1)\n",
    "        out = self.dropout(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Compact Model: Alignment Module (FROM YOUR CODE)\n",
    "# ==============================================================================\n",
    "\n",
    "class AlignmentModule(nn.Module):\n",
    "    \"\"\"\n",
    "    Alignment module inspired by DNAFormer.\n",
    "    Processes each read individually to learn alignment before NCI.\n",
    "    Lighter than DNAFormer (uses 2 conv blocks instead of 4 double_conv1D + linear).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim, out_channels, seq_len, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.conv_block1 = MultiKernelConvBlock(embed_dim, out_channels, seq_len, dropout)\n",
    "        self.conv_block2 = MultiKernelConvBlock(out_channels, out_channels, seq_len, dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, cluster_size, embed_dim, seq_len)\n",
    "        batch, cluster, emb, seq = x.shape\n",
    "        \n",
    "        # Process each read independently\n",
    "        x = x.view(batch * cluster, emb, seq)\n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "        \n",
    "        # Reshape back\n",
    "        x = x.view(batch, cluster, -1, seq)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Compact Model: Embedding Module (FROM YOUR CODE)\n",
    "# ==============================================================================\n",
    "\n",
    "class EmbeddingModule(nn.Module):\n",
    "    \"\"\"\n",
    "    Embedding module - processes cluster after NCI to extract cluster-level features.\n",
    "    Lighter than DNAFormer (uses 1 conv block + linear instead of 4 double_conv1D + linear).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, in_len, out_len, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.conv_block = MultiKernelConvBlock(in_channels, out_channels, in_len, dropout)\n",
    "        \n",
    "        # Linear projection to target length\n",
    "        self.linear = nn.Linear(in_len, out_len)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, channels, seq_len)\n",
    "        x = self.conv_block(x)  # (B, out_channels, in_len)\n",
    "        \n",
    "        # Apply linear transformation to sequence dimension\n",
    "        batch, channels, seq_len = x.shape\n",
    "        \n",
    "        # Reshape: (B, C, L) -> (B*C, L)\n",
    "        x = x.reshape(batch * channels, seq_len)\n",
    "        \n",
    "        # Apply linear: (B*C, in_len) -> (B*C, out_len)\n",
    "        x = self.linear(x)\n",
    "        \n",
    "        # Reshape back: (B*C, out_len) -> (B, C, out_len)\n",
    "        x = x.reshape(batch, channels, -1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Compact Model: Complete Model (FROM YOUR CODE)\n",
    "# ==============================================================================\n",
    "\n",
    "class ImprovedDNAReconstructionModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Improved DNA reconstruction model inspired by DNAFormer architecture.\n",
    "    \n",
    "    Architecture:\n",
    "    1. Embedding layer (nn.Embedding)\n",
    "    2. Alignment module (per-read processing with multi-kernel convs)\n",
    "    3. NCI (Non-Coherent Integration) - sum over cluster dimension\n",
    "    4. Embedding module (cluster-level feature extraction)\n",
    "    5. BiGRU (instead of Transformer for efficiency)\n",
    "    6. Output projection (single linear layer)\n",
    "    \n",
    "    Key differences from DNAFormer:\n",
    "    - NO Siamese architecture (single branch)\n",
    "    - 3 kernels {1,3,5} instead of 4 {1,3,5,7}\n",
    "    - BiGRU instead of 12-layer Transformer\n",
    "    - NO Fusion module\n",
    "    - Lighter alignment and embedding modules\n",
    "    \n",
    "    Parameters: ~5-8M (vs DNAFormer's ~100M)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, label_seq_len, max_read_len, padding_idx,\n",
    "                 embed_dim=300, alignment_filters=128, embedding_filters=500,\n",
    "                 gru_hidden=300, gru_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.label_seq_len = label_seq_len\n",
    "        self.max_read_len = max_read_len\n",
    "        \n",
    "        # 1. Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=padding_idx)\n",
    "        \n",
    "        # 2. Alignment module (per-read processing)\n",
    "        self.alignment = AlignmentModule(\n",
    "            embed_dim=embed_dim,\n",
    "            out_channels=alignment_filters,\n",
    "            seq_len=max_read_len,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # 3. NCI (Non-Coherent Integration) is just a sum - no learnable params\n",
    "        \n",
    "        # 4. Embedding module (cluster-level processing)\n",
    "        self.embedding_module = EmbeddingModule(\n",
    "            in_channels=alignment_filters,\n",
    "            out_channels=embedding_filters,\n",
    "            in_len=max_read_len,\n",
    "            out_len=label_seq_len,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # 5. BiGRU for sequence modeling\n",
    "        self.gru = nn.GRU(\n",
    "            embedding_filters,\n",
    "            gru_hidden,\n",
    "            num_layers=gru_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout if gru_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # 6. Output projection\n",
    "        self.fc_out = nn.Linear(gru_hidden * 2, vocab_size)\n",
    "    \n",
    "    def forward(self, cluster_batch):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            cluster_batch: (batch_size, max_cluster_size, max_read_len)\n",
    "        \n",
    "        Returns:\n",
    "            logits: (batch_size, label_seq_len, vocab_size)\n",
    "        \"\"\"\n",
    "        # 1. Embed all reads\n",
    "        embedded = self.embedding(cluster_batch)  # (B, cluster, read_len, embed_dim)\n",
    "        embedded = embedded.permute(0, 1, 3, 2)  # (B, cluster, embed_dim, read_len)\n",
    "        \n",
    "        # 2. Alignment module - process each read independently\n",
    "        aligned = self.alignment(embedded)  # (B, cluster, alignment_filters, read_len)\n",
    "        \n",
    "        # 3. NCI (Non-Coherent Integration) - sum over cluster dimension\n",
    "        nci_output = torch.sum(aligned, dim=1)  # (B, alignment_filters, read_len)\n",
    "        \n",
    "        # 4. Embedding module - process cluster as a whole\n",
    "        cluster_features = self.embedding_module(nci_output)  # (B, embedding_filters, label_seq_len)\n",
    "        \n",
    "        # 5. Prepare for GRU: (B, seq_len, features)\n",
    "        x = cluster_features.permute(0, 2, 1)  # (B, label_seq_len, embedding_filters)\n",
    "        \n",
    "        # 6. BiGRU\n",
    "        gru_out, _ = self.gru(x)  # (B, label_seq_len, gru_hidden*2)\n",
    "        gru_out = self.dropout(gru_out)\n",
    "        \n",
    "        # 7. Output projection\n",
    "        logits = self.fc_out(gru_out)  # (B, label_seq_len, vocab_size)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PART 3: Parameter Counting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\"Count total trainable parameters in a model.\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def count_parameters_by_module(model):\n",
    "    \"\"\"Count parameters for each named module in the model.\"\"\"\n",
    "    param_counts = OrderedDict()\n",
    "    \n",
    "    for name, module in model.named_children():\n",
    "        params = sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "        param_counts[name] = params\n",
    "    \n",
    "    param_counts['TOTAL'] = count_parameters(model)\n",
    "    return param_counts\n",
    "\n",
    "\n",
    "def format_params(n):\n",
    "    \"\"\"Format parameter count nicely.\"\"\"\n",
    "    if n >= 1e9:\n",
    "        return f\"{n/1e9:.2f}B\"\n",
    "    elif n >= 1e6:\n",
    "        return f\"{n/1e6:.2f}M\"\n",
    "    elif n >= 1e3:\n",
    "        return f\"{n/1e3:.2f}K\"\n",
    "    else:\n",
    "        return str(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PART 4: FLOPs Counting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_dnaformer_flops(enc_filters, alignment_filters, d_model, n_head,\n",
    "                          num_layers, dim_feedforward, output_ch,\n",
    "                          noisy_copies_length, label_length, \n",
    "                          batch_size=1, max_cluster_size=16):\n",
    "    \"\"\"\n",
    "    Estimate FLOPs for DNAFormer (forward pass).\n",
    "    \n",
    "    FLOPs = 2 * MACs (multiply-accumulate operations)\n",
    "    \"\"\"\n",
    "    flops = OrderedDict()\n",
    "    num_reads = batch_size * max_cluster_size\n",
    "    \n",
    "    # Helper functions\n",
    "    def ds_conv_flops(in_ch, out_ch, kernel_size, seq_len):\n",
    "        \"\"\"FLOPs for depthwise separable conv.\"\"\"\n",
    "        # Depthwise: in_ch * kernel_size * seq_len MACs\n",
    "        depthwise = in_ch * kernel_size * seq_len\n",
    "        # Pointwise: in_ch * out_ch * seq_len MACs\n",
    "        pointwise = in_ch * out_ch * seq_len\n",
    "        return 2 * (depthwise + pointwise)  # MACs -> FLOPs\n",
    "    \n",
    "    def double_conv1d_flops(in_ch, out_ch, kernel_size, seq_len):\n",
    "        \"\"\"FLOPs for double_conv1D block.\"\"\"\n",
    "        conv1 = ds_conv_flops(in_ch, out_ch, kernel_size, seq_len)\n",
    "        conv2 = ds_conv_flops(out_ch, out_ch, kernel_size, seq_len)\n",
    "        # LayerNorm: ~5 ops per element\n",
    "        norms = 2 * 5 * out_ch * seq_len\n",
    "        # GELU: ~8 ops per element (exp, mul, add, etc.)\n",
    "        activations = 2 * 8 * out_ch * seq_len\n",
    "        return conv1 + conv2 + norms + activations\n",
    "    \n",
    "    def linear_block_flops(in_len, out_len, channels):\n",
    "        \"\"\"FLOPs for linear_block.\"\"\"\n",
    "        fc1 = 2 * channels * in_len * out_len\n",
    "        fc2 = 2 * channels * out_len * out_len\n",
    "        fc3 = 2 * channels * out_len * out_len\n",
    "        norms = 2 * 5 * channels * out_len\n",
    "        activations = 2 * 8 * channels * out_len\n",
    "        return fc1 + fc2 + fc3 + norms + activations\n",
    "    \n",
    "    # =========================================================================\n",
    "    # 1. ALIGNMENT MODULE\n",
    "    # =========================================================================\n",
    "    align_out_ch = alignment_filters // 4\n",
    "    \n",
    "    align_conv1 = double_conv1d_flops(enc_filters, align_out_ch, 1, noisy_copies_length) * num_reads\n",
    "    align_conv3 = double_conv1d_flops(enc_filters, align_out_ch, 3, noisy_copies_length) * num_reads\n",
    "    align_conv5 = double_conv1d_flops(enc_filters, align_out_ch, 5, noisy_copies_length) * num_reads\n",
    "    align_conv7 = double_conv1d_flops(enc_filters, align_out_ch, 7, noisy_copies_length) * num_reads\n",
    "    align_linear = linear_block_flops(noisy_copies_length, noisy_copies_length, alignment_filters) * num_reads\n",
    "    \n",
    "    flops['Alignment_Conv_K1'] = align_conv1\n",
    "    flops['Alignment_Conv_K3'] = align_conv3\n",
    "    flops['Alignment_Conv_K5'] = align_conv5\n",
    "    flops['Alignment_Conv_K7'] = align_conv7\n",
    "    flops['Alignment_Linear'] = align_linear\n",
    "    flops['Alignment_Module_Total'] = align_conv1 + align_conv3 + align_conv5 + align_conv7 + align_linear\n",
    "    \n",
    "    # =========================================================================\n",
    "    # 2. EMBEDDING MODULE (after NCI)\n",
    "    # =========================================================================\n",
    "    emb_out_ch = d_model // 4\n",
    "    \n",
    "    emb_conv1 = double_conv1d_flops(alignment_filters, emb_out_ch, 1, noisy_copies_length) * batch_size\n",
    "    emb_conv3 = double_conv1d_flops(alignment_filters, emb_out_ch, 3, noisy_copies_length) * batch_size\n",
    "    emb_conv5 = double_conv1d_flops(alignment_filters, emb_out_ch, 5, noisy_copies_length) * batch_size\n",
    "    emb_conv7 = double_conv1d_flops(alignment_filters, emb_out_ch, 7, noisy_copies_length) * batch_size\n",
    "    emb_linear = linear_block_flops(noisy_copies_length, label_length, d_model) * batch_size\n",
    "    \n",
    "    flops['Embedding_Conv_K1'] = emb_conv1\n",
    "    flops['Embedding_Conv_K3'] = emb_conv3\n",
    "    flops['Embedding_Conv_K5'] = emb_conv5\n",
    "    flops['Embedding_Conv_K7'] = emb_conv7\n",
    "    flops['Embedding_Linear'] = emb_linear\n",
    "    flops['Embedding_Module_Total'] = emb_conv1 + emb_conv3 + emb_conv5 + emb_conv7 + emb_linear\n",
    "    \n",
    "    # =========================================================================\n",
    "    # 3. TRANSFORMER ENCODER\n",
    "    # =========================================================================\n",
    "    seq_len = label_length\n",
    "    \n",
    "    # Self-attention per layer\n",
    "    # Q, K, V projections: 3 * 2 * batch * seq * d_model^2\n",
    "    qkv_proj = 3 * 2 * batch_size * seq_len * d_model * d_model\n",
    "    # Attention scores: batch * n_head * seq * seq * (d_model/n_head)\n",
    "    attn_scores = 2 * batch_size * n_head * seq_len * seq_len * (d_model // n_head)\n",
    "    # Softmax: ~5 ops per element\n",
    "    softmax = 5 * batch_size * n_head * seq_len * seq_len\n",
    "    # Attention output: batch * n_head * seq * (d_model/n_head) * seq\n",
    "    attn_out = 2 * batch_size * n_head * seq_len * (d_model // n_head) * seq_len\n",
    "    # Output projection: batch * seq * d_model^2\n",
    "    out_proj = 2 * batch_size * seq_len * d_model * d_model\n",
    "    \n",
    "    attn_total = qkv_proj + attn_scores + softmax + attn_out + out_proj\n",
    "    \n",
    "    # FFN per layer: d_model -> dim_feedforward -> d_model\n",
    "    ffn1 = 2 * batch_size * seq_len * d_model * dim_feedforward\n",
    "    ffn2 = 2 * batch_size * seq_len * dim_feedforward * d_model\n",
    "    ffn_total = ffn1 + ffn2\n",
    "    \n",
    "    # Layer norms\n",
    "    layer_norms = 2 * 5 * batch_size * seq_len * d_model\n",
    "    \n",
    "    transformer_per_layer = attn_total + ffn_total + layer_norms\n",
    "    \n",
    "    flops['Transformer_Attention_per_layer'] = attn_total\n",
    "    flops['Transformer_FFN_per_layer'] = ffn_total\n",
    "    flops['Transformer_LayerNorm_per_layer'] = layer_norms\n",
    "    flops['Transformer_per_layer_Total'] = transformer_per_layer\n",
    "    flops['Transformer_Total'] = transformer_per_layer * num_layers\n",
    "    \n",
    "    # =========================================================================\n",
    "    # 4. OUTPUT MODULE\n",
    "    # =========================================================================\n",
    "    out_conv1 = 2 * batch_size * d_model * d_model * label_length\n",
    "    out_conv2 = 2 * batch_size * d_model * d_model * label_length\n",
    "    out_conv3 = 2 * batch_size * d_model * output_ch * label_length\n",
    "    \n",
    "    flops['Output_Conv1'] = out_conv1\n",
    "    flops['Output_Conv2'] = out_conv2\n",
    "    flops['Output_Conv3'] = out_conv3\n",
    "    flops['Output_Module_Total'] = out_conv1 + out_conv2 + out_conv3\n",
    "    \n",
    "    # =========================================================================\n",
    "    # 5. FUSION MODULE\n",
    "    # =========================================================================\n",
    "    fusion_conv1 = 2 * batch_size * output_ch * output_ch * label_length\n",
    "    fusion_conv2 = 2 * batch_size * output_ch * output_ch * label_length\n",
    "    fusion_conv3 = 2 * batch_size * output_ch * output_ch * label_length\n",
    "    fusion_elemwise = batch_size * output_ch * label_length * 4  # mul, mul, add, div\n",
    "    \n",
    "    flops['Fusion_Conv1'] = fusion_conv1\n",
    "    flops['Fusion_Conv2'] = fusion_conv2\n",
    "    flops['Fusion_Conv3'] = fusion_conv3\n",
    "    flops['Fusion_Elementwise'] = fusion_elemwise\n",
    "    flops['Fusion_Module_Total'] = fusion_conv1 + fusion_conv2 + fusion_conv3 + fusion_elemwise\n",
    "    \n",
    "    # =========================================================================\n",
    "    # GRAND TOTAL\n",
    "    # =========================================================================\n",
    "    flops['GRAND_TOTAL'] = (flops['Alignment_Module_Total'] + \n",
    "                            flops['Embedding_Module_Total'] + \n",
    "                            flops['Transformer_Total'] + \n",
    "                            flops['Output_Module_Total'] + \n",
    "                            flops['Fusion_Module_Total'])\n",
    "    \n",
    "    return flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_compact_flops(vocab_size, embed_dim, alignment_filters, \n",
    "                        embedding_filters, gru_hidden, gru_layers,\n",
    "                        max_read_len, label_seq_len,\n",
    "                        batch_size=1, max_cluster_size=16):\n",
    "    \"\"\"\n",
    "    Estimate FLOPs for Compact BiGRU Model (forward pass).\n",
    "    \"\"\"\n",
    "    flops = OrderedDict()\n",
    "    num_reads = batch_size * max_cluster_size\n",
    "    \n",
    "    # Helper functions\n",
    "    def ds_conv_flops(in_ch, out_ch, kernel_size, seq_len):\n",
    "        depthwise = in_ch * kernel_size * seq_len\n",
    "        pointwise = in_ch * out_ch * seq_len\n",
    "        return 2 * (depthwise + pointwise)\n",
    "    \n",
    "    def multi_kernel_block_flops(in_ch, out_ch, seq_len):\n",
    "        c1 = out_ch // 3\n",
    "        c2 = out_ch // 3\n",
    "        c3 = out_ch - c1 - c2\n",
    "        \n",
    "        conv1 = ds_conv_flops(in_ch, c1, 1, seq_len)\n",
    "        conv3 = ds_conv_flops(in_ch, c2, 3, seq_len)\n",
    "        conv5 = ds_conv_flops(in_ch, c3, 5, seq_len)\n",
    "        \n",
    "        # LayerNorms: 5 ops per element\n",
    "        norms = 5 * (c1 * seq_len + c2 * seq_len + c3 * seq_len)\n",
    "        # GELU: 8 ops per element\n",
    "        activations = 8 * out_ch * seq_len\n",
    "        \n",
    "        return conv1 + conv3 + conv5 + norms + activations\n",
    "    \n",
    "    # =========================================================================\n",
    "    # 1. EMBEDDING LAYER\n",
    "    # =========================================================================\n",
    "    # Lookup: essentially 0 FLOPs (just memory access)\n",
    "    flops['Embedding_Layer'] = 0\n",
    "    \n",
    "    # =========================================================================\n",
    "    # 2. ALIGNMENT MODULE\n",
    "    # =========================================================================\n",
    "    align_block1 = multi_kernel_block_flops(embed_dim, alignment_filters, max_read_len) * num_reads\n",
    "    align_block2 = multi_kernel_block_flops(alignment_filters, alignment_filters, max_read_len) * num_reads\n",
    "    \n",
    "    flops['Alignment_Block1'] = align_block1\n",
    "    flops['Alignment_Block2'] = align_block2\n",
    "    flops['Alignment_Module_Total'] = align_block1 + align_block2\n",
    "    \n",
    "    # =========================================================================\n",
    "    # 3. NCI (Sum)\n",
    "    # =========================================================================\n",
    "    flops['NCI'] = batch_size * max_cluster_size * alignment_filters * max_read_len\n",
    "    \n",
    "    # =========================================================================\n",
    "    # 4. EMBEDDING MODULE\n",
    "    # =========================================================================\n",
    "    emb_conv = multi_kernel_block_flops(alignment_filters, embedding_filters, max_read_len) * batch_size\n",
    "    emb_linear = 2 * batch_size * embedding_filters * max_read_len * label_seq_len\n",
    "    \n",
    "    flops['Embedding_ConvBlock'] = emb_conv\n",
    "    flops['Embedding_Linear'] = emb_linear\n",
    "    flops['Embedding_Module_Total'] = emb_conv + emb_linear\n",
    "    \n",
    "    # =========================================================================\n",
    "    # 5. BiGRU\n",
    "    # =========================================================================\n",
    "    # GRU FLOPs per timestep per layer per direction:\n",
    "    # 3 gates, each: 2 * (input_size * hidden_size + hidden_size * hidden_size)\n",
    "    # Plus activations: ~10 ops per hidden unit for sigmoid/tanh\n",
    "    \n",
    "    gru_flops = 0\n",
    "    for layer in range(gru_layers):\n",
    "        if layer == 0:\n",
    "            input_size = embedding_filters\n",
    "        else:\n",
    "            input_size = 2 * gru_hidden  # bidirectional output\n",
    "        \n",
    "        # Per timestep per direction\n",
    "        per_timestep = 3 * 2 * (input_size * gru_hidden + gru_hidden * gru_hidden)\n",
    "        # Activations\n",
    "        per_timestep += 3 * 10 * gru_hidden  # sigmoid/tanh for 3 gates\n",
    "        # Both directions\n",
    "        per_timestep *= 2\n",
    "        # All timesteps, all batches\n",
    "        gru_flops += batch_size * label_seq_len * per_timestep\n",
    "    \n",
    "    flops['BiGRU_Total'] = gru_flops\n",
    "    \n",
    "    # =========================================================================\n",
    "    # 6. OUTPUT PROJECTION\n",
    "    # =========================================================================\n",
    "    flops['Output_Linear'] = 2 * batch_size * label_seq_len * (2 * gru_hidden) * vocab_size\n",
    "    \n",
    "    # =========================================================================\n",
    "    # GRAND TOTAL\n",
    "    # =========================================================================\n",
    "    flops['GRAND_TOTAL'] = (flops['Embedding_Layer'] + \n",
    "                            flops['Alignment_Module_Total'] + \n",
    "                            flops['NCI'] +\n",
    "                            flops['Embedding_Module_Total'] + \n",
    "                            flops['BiGRU_Total'] + \n",
    "                            flops['Output_Linear'])\n",
    "    \n",
    "    return flops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PART 5: Run Analysis for All Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNAFormer Config:\n",
      "  enc_filters: 4\n",
      "  alignment_filters: 128\n",
      "  d_model: 1024\n",
      "  n_head: 32\n",
      "  num_layers: 12\n",
      "  dim_feedforward: 2048\n",
      "  output_ch: 4\n",
      "  p_dropout: 0\n",
      "\n",
      "Compact Model Config:\n",
      "  vocab_size: 5\n",
      "  embed_dim: 300\n",
      "  alignment_filters: 128\n",
      "  embedding_filters: 500\n",
      "  gru_hidden: 300\n",
      "  gru_layers: 2\n",
      "  dropout: 0.1\n",
      "  padding_idx: 0\n"
     ]
    }
   ],
   "source": [
    "# DNAFormer fixed hyperparameters (from paper)\n",
    "DNAFORMER_CONFIG = {\n",
    "    'enc_filters': 4,          # One-hot encoding of DNA (A, C, G, T)\n",
    "    'alignment_filters': 128,\n",
    "    'd_model': 1024,\n",
    "    'n_head': 32,\n",
    "    'num_layers': 12,\n",
    "    'dim_feedforward': 2048,\n",
    "    'output_ch': 4,\n",
    "    'p_dropout': 0,\n",
    "}\n",
    "\n",
    "# Your compact model hyperparameters (from your code)\n",
    "COMPACT_CONFIG = {\n",
    "    'vocab_size': 5,           # N, A, C, G, T\n",
    "    'embed_dim': 300,\n",
    "    'alignment_filters': 128,\n",
    "    'embedding_filters': 500,\n",
    "    'gru_hidden': 300,\n",
    "    'gru_layers': 2,\n",
    "    'dropout': 0.1,\n",
    "    'padding_idx': 0,\n",
    "}\n",
    "\n",
    "MAX_CLUSTER_SIZE = 16  # From DNAFormer paper\n",
    "\n",
    "print(\"DNAFormer Config:\")\n",
    "for k, v in DNAFORMER_CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "print(\"\\nCompact Model Config:\")\n",
    "for k, v in COMPACT_CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "PARAMETER AND FLOPS ANALYSIS\n",
      "====================================================================================================\n",
      "\n",
      "================================================================================\n",
      "DATASET: Srinivasavaradhan\n",
      "================================================================================\n",
      "Description: Srinivasavaradhan et al. - Twist, MinION (high error)\n",
      "\n",
      "Derived parameters:\n",
      "  label_length (output): 110\n",
      "  noisy_copies_length (input): 120\n",
      "  max_read_len: 128\n",
      "\n",
      "--- DNAFormer ---\n",
      "  Total Parameters: 103,396,622 (103.40M)\n",
      "  Total FLOPs:      23,627,898,400 (23.63B)\n",
      "\n",
      "--- Compact BiGRU (Your Model) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/DATA/shubham19/anaconda3/envs/pytorchenv/lib/python3.10/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Total Parameters: 3,405,643 (3.41M)\n",
      "  Total FLOPs:      956,661,856 (956.66M)\n",
      "\n",
      "--- Comparison ---\n",
      "  Parameter Reduction: 96.7%\n",
      "  FLOP Reduction:      96.0%\n",
      "\n",
      "================================================================================\n",
      "DATASET: Grass\n",
      "================================================================================\n",
      "Description: Grass et al. (2015) - CustomArray, Illumina miSeq\n",
      "\n",
      "Derived parameters:\n",
      "  label_length (output): 117\n",
      "  noisy_copies_length (input): 128\n",
      "  max_read_len: 136\n",
      "\n",
      "--- DNAFormer ---\n",
      "  Total Parameters: 103,407,903 (103.41M)\n",
      "  Total FLOPs:      25,192,109,744 (25.19B)\n",
      "\n",
      "--- Compact BiGRU (Your Model) ---\n",
      "  Total Parameters: 3,419,578 (3.42M)\n",
      "  Total FLOPs:      1,018,175,472 (1.02B)\n",
      "\n",
      "--- Comparison ---\n",
      "  Parameter Reduction: 96.7%\n",
      "  FLOP Reduction:      96.0%\n",
      "\n",
      "================================================================================\n",
      "DATASET: Erlich\n",
      "================================================================================\n",
      "Description: Erlich et al. (2017) - DNA Fountain, Illumina miSeq\n",
      "\n",
      "Derived parameters:\n",
      "  label_length (output): 152\n",
      "  noisy_copies_length (input): 162\n",
      "  max_read_len: 170\n",
      "\n",
      "--- DNAFormer ---\n",
      "  Total Parameters: 103,467,602 (103.47M)\n",
      "  Total FLOPs:      33,088,512,640 (33.09B)\n",
      "\n",
      "--- Compact BiGRU (Your Model) ---\n",
      "  Total Parameters: 3,480,949 (3.48M)\n",
      "  Total FLOPs:      1,314,100,840 (1.31B)\n",
      "\n",
      "--- Comparison ---\n",
      "  Parameter Reduction: 96.6%\n",
      "  FLOP Reduction:      96.0%\n",
      "\n",
      "================================================================================\n",
      "DATASET: BinnedNanoporeTwoFlowcells\n",
      "================================================================================\n",
      "Description: DNAformer Nanopore Two Flowcells Combined\n",
      "\n",
      "Derived parameters:\n",
      "  label_length (output): 116\n",
      "  noisy_copies_length (input): 120\n",
      "  max_read_len: 140\n",
      "\n",
      "--- DNAFormer ---\n",
      "  Total Parameters: 103,400,108 (103.40M)\n",
      "  Total FLOPs:      24,938,252,992 (24.94B)\n",
      "\n",
      "--- Compact BiGRU (Your Model) ---\n",
      "  Total Parameters: 3,425,953 (3.43M)\n",
      "  Total FLOPs:      1,020,615,280 (1.02B)\n",
      "\n",
      "--- Comparison ---\n",
      "  Parameter Reduction: 96.7%\n",
      "  FLOP Reduction:      95.9%\n",
      "\n",
      "================================================================================\n",
      "DATASET: BinnedTestIllumina\n",
      "================================================================================\n",
      "Description: DNAformer Test Illumina - Twist + miSeq\n",
      "\n",
      "Derived parameters:\n",
      "  label_length (output): 116\n",
      "  noisy_copies_length (input): 120\n",
      "  max_read_len: 140\n",
      "\n",
      "--- DNAFormer ---\n",
      "  Total Parameters: 103,400,108 (103.40M)\n",
      "  Total FLOPs:      24,938,252,992 (24.94B)\n",
      "\n",
      "--- Compact BiGRU (Your Model) ---\n",
      "  Total Parameters: 3,425,953 (3.43M)\n",
      "  Total FLOPs:      1,020,615,280 (1.02B)\n",
      "\n",
      "--- Comparison ---\n",
      "  Parameter Reduction: 96.7%\n",
      "  FLOP Reduction:      95.9%\n"
     ]
    }
   ],
   "source": [
    "# Storage for results\n",
    "results_summary = []\n",
    "dnaformer_detailed_results = {}\n",
    "compact_detailed_results = {}\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"PARAMETER AND FLOPS ANALYSIS\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "for dataset_name, config in DATASET_CONFIGS.items():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"DATASET: {dataset_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Description: {config['description']}\")\n",
    "    \n",
    "    # Calculate derived parameters\n",
    "    label_length = config['label_len']\n",
    "    if config.get('index_length', 0) > 0:\n",
    "        label_length -= config['index_length']\n",
    "    \n",
    "    noisy_copies_length = config['label_len'] + config['max_deviation']\n",
    "    if config.get('index_length', 0) > 0:\n",
    "        noisy_copies_length -= config['index_length']\n",
    "    \n",
    "    max_read_len = config['label_len'] + config['max_deviation'] + 8  # Extra buffer\n",
    "    \n",
    "    print(f\"\\nDerived parameters:\")\n",
    "    print(f\"  label_length (output): {label_length}\")\n",
    "    print(f\"  noisy_copies_length (input): {noisy_copies_length}\")\n",
    "    print(f\"  max_read_len: {max_read_len}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # DNAFormer\n",
    "    # =========================================================================\n",
    "    print(f\"\\n--- DNAFormer ---\")\n",
    "    \n",
    "    # Build model\n",
    "    dnaformer_model = DNAFormer(\n",
    "        enc_filters=DNAFORMER_CONFIG['enc_filters'],\n",
    "        alignment_filters=DNAFORMER_CONFIG['alignment_filters'],\n",
    "        d_model=DNAFORMER_CONFIG['d_model'],\n",
    "        n_head=DNAFORMER_CONFIG['n_head'],\n",
    "        num_layers=DNAFORMER_CONFIG['num_layers'],\n",
    "        dim_feedforward=DNAFORMER_CONFIG['dim_feedforward'],\n",
    "        output_ch=DNAFORMER_CONFIG['output_ch'],\n",
    "        noisy_copies_length=noisy_copies_length,\n",
    "        label_length=label_length,\n",
    "        p_dropout=DNAFORMER_CONFIG['p_dropout']\n",
    "    )\n",
    "    \n",
    "    # Count parameters\n",
    "    dnaformer_params = count_parameters_by_module(dnaformer_model)\n",
    "    \n",
    "    # Count FLOPs\n",
    "    dnaformer_flops = count_dnaformer_flops(\n",
    "        enc_filters=DNAFORMER_CONFIG['enc_filters'],\n",
    "        alignment_filters=DNAFORMER_CONFIG['alignment_filters'],\n",
    "        d_model=DNAFORMER_CONFIG['d_model'],\n",
    "        n_head=DNAFORMER_CONFIG['n_head'],\n",
    "        num_layers=DNAFORMER_CONFIG['num_layers'],\n",
    "        dim_feedforward=DNAFORMER_CONFIG['dim_feedforward'],\n",
    "        output_ch=DNAFORMER_CONFIG['output_ch'],\n",
    "        noisy_copies_length=noisy_copies_length,\n",
    "        label_length=label_length,\n",
    "        batch_size=1,\n",
    "        max_cluster_size=MAX_CLUSTER_SIZE\n",
    "    )\n",
    "    \n",
    "    dnaformer_detailed_results[dataset_name] = {\n",
    "        'params': dnaformer_params,\n",
    "        'flops': dnaformer_flops\n",
    "    }\n",
    "    \n",
    "    print(f\"  Total Parameters: {dnaformer_params['TOTAL']:,} ({format_params(dnaformer_params['TOTAL'])})\")\n",
    "    print(f\"  Total FLOPs:      {dnaformer_flops['GRAND_TOTAL']:,} ({format_params(dnaformer_flops['GRAND_TOTAL'])})\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # Compact BiGRU\n",
    "    # =========================================================================\n",
    "    print(f\"\\n--- Compact BiGRU (Your Model) ---\")\n",
    "    \n",
    "    # Build model\n",
    "    compact_model = ImprovedDNAReconstructionModel(\n",
    "        vocab_size=COMPACT_CONFIG['vocab_size'],\n",
    "        label_seq_len=label_length,\n",
    "        max_read_len=max_read_len,\n",
    "        padding_idx=COMPACT_CONFIG['padding_idx'],\n",
    "        embed_dim=COMPACT_CONFIG['embed_dim'],\n",
    "        alignment_filters=COMPACT_CONFIG['alignment_filters'],\n",
    "        embedding_filters=COMPACT_CONFIG['embedding_filters'],\n",
    "        gru_hidden=COMPACT_CONFIG['gru_hidden'],\n",
    "        gru_layers=COMPACT_CONFIG['gru_layers'],\n",
    "        dropout=COMPACT_CONFIG['dropout']\n",
    "    )\n",
    "    \n",
    "    # Count parameters\n",
    "    compact_params = count_parameters_by_module(compact_model)\n",
    "    \n",
    "    # Count FLOPs\n",
    "    compact_flops = count_compact_flops(\n",
    "        vocab_size=COMPACT_CONFIG['vocab_size'],\n",
    "        embed_dim=COMPACT_CONFIG['embed_dim'],\n",
    "        alignment_filters=COMPACT_CONFIG['alignment_filters'],\n",
    "        embedding_filters=COMPACT_CONFIG['embedding_filters'],\n",
    "        gru_hidden=COMPACT_CONFIG['gru_hidden'],\n",
    "        gru_layers=COMPACT_CONFIG['gru_layers'],\n",
    "        max_read_len=max_read_len,\n",
    "        label_seq_len=label_length,\n",
    "        batch_size=1,\n",
    "        max_cluster_size=MAX_CLUSTER_SIZE\n",
    "    )\n",
    "    \n",
    "    compact_detailed_results[dataset_name] = {\n",
    "        'params': compact_params,\n",
    "        'flops': compact_flops\n",
    "    }\n",
    "    \n",
    "    print(f\"  Total Parameters: {compact_params['TOTAL']:,} ({format_params(compact_params['TOTAL'])})\")\n",
    "    print(f\"  Total FLOPs:      {compact_flops['GRAND_TOTAL']:,} ({format_params(compact_flops['GRAND_TOTAL'])})\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # Comparison\n",
    "    # =========================================================================\n",
    "    param_reduction = (1 - compact_params['TOTAL'] / dnaformer_params['TOTAL']) * 100\n",
    "    flop_reduction = (1 - compact_flops['GRAND_TOTAL'] / dnaformer_flops['GRAND_TOTAL']) * 100\n",
    "    \n",
    "    print(f\"\\n--- Comparison ---\")\n",
    "    print(f\"  Parameter Reduction: {param_reduction:.1f}%\")\n",
    "    print(f\"  FLOP Reduction:      {flop_reduction:.1f}%\")\n",
    "    \n",
    "    # Store summary\n",
    "    results_summary.append({\n",
    "        'Dataset': dataset_name,\n",
    "        'Label_Length': label_length,\n",
    "        'Noisy_Copies_Len': noisy_copies_length,\n",
    "        'Max_Read_Len': max_read_len,\n",
    "        'DNAFormer_Params': dnaformer_params['TOTAL'],\n",
    "        'DNAFormer_Params_M': dnaformer_params['TOTAL'] / 1e6,\n",
    "        'DNAFormer_FLOPs': dnaformer_flops['GRAND_TOTAL'],\n",
    "        'DNAFormer_FLOPs_G': dnaformer_flops['GRAND_TOTAL'] / 1e9,\n",
    "        'Compact_Params': compact_params['TOTAL'],\n",
    "        'Compact_Params_M': compact_params['TOTAL'] / 1e6,\n",
    "        'Compact_FLOPs': compact_flops['GRAND_TOTAL'],\n",
    "        'Compact_FLOPs_G': compact_flops['GRAND_TOTAL'] / 1e9,\n",
    "        'Param_Reduction_%': param_reduction,\n",
    "        'FLOP_Reduction_%': flop_reduction,\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PART 6: Summary Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "SUMMARY TABLE\n",
      "====================================================================================================\n",
      "\n",
      "                   Dataset  Label Len  DNAFormer (M)  Compact (M)  Param Red. %  DNAFormer (G)  Compact (G)  FLOP Red. %\n",
      "         Srinivasavaradhan        110         103.40         3.41          96.7          23.63        0.957         96.0\n",
      "                     Grass        117         103.41         3.42          96.7          25.19        1.018         96.0\n",
      "                    Erlich        152         103.47         3.48          96.6          33.09        1.314         96.0\n",
      "BinnedNanoporeTwoFlowcells        116         103.40         3.43          96.7          24.94        1.021         95.9\n",
      "        BinnedTestIllumina        116         103.40         3.43          96.7          24.94        1.021         95.9\n"
     ]
    }
   ],
   "source": [
    "# Create summary dataframe\n",
    "df_summary = pd.DataFrame(results_summary)\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"SUMMARY TABLE\")\n",
    "print(\"=\" * 100)\n",
    "print()\n",
    "\n",
    "# Display nicely formatted\n",
    "display_cols = ['Dataset', 'Label_Length', 'DNAFormer_Params_M', 'Compact_Params_M', \n",
    "                'Param_Reduction_%', 'DNAFormer_FLOPs_G', 'Compact_FLOPs_G', 'FLOP_Reduction_%']\n",
    "\n",
    "df_display = df_summary[display_cols].copy()\n",
    "df_display.columns = ['Dataset', 'Label Len', 'DNAFormer (M)', 'Compact (M)', \n",
    "                      'Param Red. %', 'DNAFormer (G)', 'Compact (G)', 'FLOP Red. %']\n",
    "\n",
    "# Round for display\n",
    "df_display['DNAFormer (M)'] = df_display['DNAFormer (M)'].round(2)\n",
    "df_display['Compact (M)'] = df_display['Compact (M)'].round(2)\n",
    "df_display['Param Red. %'] = df_display['Param Red. %'].round(1)\n",
    "df_display['DNAFormer (G)'] = df_display['DNAFormer (G)'].round(2)\n",
    "df_display['Compact (G)'] = df_display['Compact (G)'].round(3)\n",
    "df_display['FLOP Red. %'] = df_display['FLOP Red. %'].round(1)\n",
    "\n",
    "print(df_display.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "DNAFormer MODULE-BY-MODULE PARAMETERS\n",
      "====================================================================================================\n",
      "                           Alignment Embedding Transformer Output Fusion    TOTAL\n",
      "Srinivasavaradhan              0.05M     0.44M     100.80M  2.10M  0.00M  103.40M\n",
      "Grass                          0.06M     0.45M     100.80M  2.10M  0.00M  103.41M\n",
      "Erlich                         0.09M     0.48M     100.80M  2.10M  0.00M  103.47M\n",
      "BinnedNanoporeTwoFlowcells     0.05M     0.45M     100.80M  2.10M  0.00M  103.40M\n",
      "BinnedTestIllumina             0.05M     0.45M     100.80M  2.10M  0.00M  103.40M\n"
     ]
    }
   ],
   "source": [
    "# DNAFormer module-by-module parameters\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"DNAFormer MODULE-BY-MODULE PARAMETERS\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "dnaformer_module_data = {}\n",
    "for dataset_name in DATASET_CONFIGS.keys():\n",
    "    params = dnaformer_detailed_results[dataset_name]['params']\n",
    "    dnaformer_module_data[dataset_name] = {\n",
    "        'Alignment': params.get('alignement', 0),\n",
    "        'Embedding': params.get('embedding', 0),\n",
    "        'Transformer': params.get('encoder', 0),\n",
    "        'Output': params.get('output_module', 0),\n",
    "        'Fusion': params.get('fusion', 0),\n",
    "        'TOTAL': params.get('TOTAL', 0)\n",
    "    }\n",
    "\n",
    "df_dnaformer_modules = pd.DataFrame(dnaformer_module_data).T\n",
    "df_dnaformer_modules = df_dnaformer_modules.applymap(lambda x: f\"{x/1e6:.2f}M\" if x > 0 else \"0\")\n",
    "print(df_dnaformer_modules.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "COMPACT BiGRU MODULE-BY-MODULE PARAMETERS\n",
      "====================================================================================================\n",
      "                           Embedding_Layer Alignment Embedding_Module  BiGRU Output  TOTAL\n",
      "Srinivasavaradhan                     1.5K    125.7K           208.2K  3.07M   3.0K  3.41M\n",
      "Grass                                 1.5K    129.8K           218.1K  3.07M   3.0K  3.42M\n",
      "Erlich                                1.5K    147.2K           262.0K  3.07M   3.0K  3.48M\n",
      "BinnedNanoporeTwoFlowcells            1.5K    131.9K           222.4K  3.07M   3.0K  3.43M\n",
      "BinnedTestIllumina                    1.5K    131.9K           222.4K  3.07M   3.0K  3.43M\n"
     ]
    }
   ],
   "source": [
    "# Compact model module-by-module parameters\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"COMPACT BiGRU MODULE-BY-MODULE PARAMETERS\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "compact_module_data = {}\n",
    "for dataset_name in DATASET_CONFIGS.keys():\n",
    "    params = compact_detailed_results[dataset_name]['params']\n",
    "    compact_module_data[dataset_name] = {\n",
    "        'Embedding_Layer': params.get('embedding', 0),\n",
    "        'Alignment': params.get('alignment', 0),\n",
    "        'Embedding_Module': params.get('embedding_module', 0),\n",
    "        'BiGRU': params.get('gru', 0),\n",
    "        'Output': params.get('fc_out', 0),\n",
    "        'TOTAL': params.get('TOTAL', 0)\n",
    "    }\n",
    "\n",
    "df_compact_modules = pd.DataFrame(compact_module_data).T\n",
    "df_compact_modules = df_compact_modules.applymap(lambda x: f\"{x/1e6:.2f}M\" if x > 1e6 else f\"{x/1e3:.1f}K\" if x > 0 else \"0\")\n",
    "print(df_compact_modules.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "DNAFormer MODULE-BY-MODULE FLOPs\n",
      "====================================================================================================\n",
      "                           Alignment Embedding Transformer Output Fusion   TOTAL\n",
      "Srinivasavaradhan              0.21G     0.18G      22.78G  0.46G  0.00G  23.63G\n",
      "Grass                          0.24G     0.20G      24.27G  0.49G  0.00G  25.19G\n",
      "Erlich                         0.37G     0.28G      31.80G  0.64G  0.00G  33.09G\n",
      "BinnedNanoporeTwoFlowcells     0.21G     0.19G      24.06G  0.49G  0.00G  24.94G\n",
      "BinnedTestIllumina             0.21G     0.19G      24.06G  0.49G  0.00G  24.94G\n"
     ]
    }
   ],
   "source": [
    "# DNAFormer module-by-module FLOPs\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"DNAFormer MODULE-BY-MODULE FLOPs\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "dnaformer_flop_data = {}\n",
    "for dataset_name in DATASET_CONFIGS.keys():\n",
    "    flops = dnaformer_detailed_results[dataset_name]['flops']\n",
    "    dnaformer_flop_data[dataset_name] = {\n",
    "        'Alignment': flops.get('Alignment_Module_Total', 0),\n",
    "        'Embedding': flops.get('Embedding_Module_Total', 0),\n",
    "        'Transformer': flops.get('Transformer_Total', 0),\n",
    "        'Output': flops.get('Output_Module_Total', 0),\n",
    "        'Fusion': flops.get('Fusion_Module_Total', 0),\n",
    "        'TOTAL': flops.get('GRAND_TOTAL', 0)\n",
    "    }\n",
    "\n",
    "df_dnaformer_flops = pd.DataFrame(dnaformer_flop_data).T\n",
    "df_dnaformer_flops = df_dnaformer_flops.applymap(lambda x: f\"{x/1e9:.2f}G\")\n",
    "print(df_dnaformer_flops.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "COMPACT BiGRU MODULE-BY-MODULE FLOPs\n",
      "====================================================================================================\n",
      "                           Embedding Alignment      NCI Embedding_Mod    BiGRU   Output    TOTAL\n",
      "Srinivasavaradhan                  0   0.2470G  0.0003G       0.0316G  0.6772G  0.0007G  0.9567G\n",
      "Grass                              0   0.2624G  0.0003G       0.0345G  0.7203G  0.0007G  1.0182G\n",
      "Erlich                             0   0.3280G  0.0003G       0.0491G  0.9357G  0.0009G  1.3141G\n",
      "BinnedNanoporeTwoFlowcells         0   0.2701G  0.0003G       0.0354G  0.7141G  0.0007G  1.0206G\n",
      "BinnedTestIllumina                 0   0.2701G  0.0003G       0.0354G  0.7141G  0.0007G  1.0206G\n"
     ]
    }
   ],
   "source": [
    "# Compact model module-by-module FLOPs\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"COMPACT BiGRU MODULE-BY-MODULE FLOPs\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "compact_flop_data = {}\n",
    "for dataset_name in DATASET_CONFIGS.keys():\n",
    "    flops = compact_detailed_results[dataset_name]['flops']\n",
    "    compact_flop_data[dataset_name] = {\n",
    "        'Embedding': flops.get('Embedding_Layer', 0),\n",
    "        'Alignment': flops.get('Alignment_Module_Total', 0),\n",
    "        'NCI': flops.get('NCI', 0),\n",
    "        'Embedding_Mod': flops.get('Embedding_Module_Total', 0),\n",
    "        'BiGRU': flops.get('BiGRU_Total', 0),\n",
    "        'Output': flops.get('Output_Linear', 0),\n",
    "        'TOTAL': flops.get('GRAND_TOTAL', 0)\n",
    "    }\n",
    "\n",
    "df_compact_flops = pd.DataFrame(compact_flop_data).T\n",
    "df_compact_flops = df_compact_flops.applymap(lambda x: f\"{x/1e9:.4f}G\" if x > 0 else \"0\")\n",
    "print(df_compact_flops.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PART 7: Architectural Differences Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "ARCHITECTURAL DIFFERENCES SUMMARY\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "           Feature                       DNAFormer                 Compact BiGRU          \n",
      "\n",
      " Sequence Modeling                 12-layer Transformer        2-layer BiGRU              \n",
      " Attention Heads                   32                          N/A                        \n",
      " Model Dimension (d_model)         1024                        500 (embedding_filters)    \n",
      " Feedforward Dimension             2048                        N/A                        \n",
      " Alignment Kernels                 4 kernels {1,3,5,7}         3 kernels {1,3,5}          \n",
      " Alignment Structure               4 double_conv1D + linear    2 multi-kernel blocks      \n",
      " Embedding Structure               4 double_conv1D + linear    1 multi-kernel + linear    \n",
      " Siamese Architecture              YES (2 branches)            NO (single branch)         \n",
      " Fusion Module                     YES (vectors + 3 conv)      NO                         \n",
      " Output Module                     3 Conv1x1 layers            1 Linear layer             \n",
      " Input Encoding                    One-hot (4 channels)        nn.Embedding (300 dim)     \n",
      " Attention Complexity              O(L)                       O(L) linear                \n",
      " ~Parameters                       ~100M                       ~5-8M                      \n",
      "\n",
      "\n",
      "KEY ADVANTAGES OF COMPACT MODEL:\n",
      "\n",
      "1. NO Transformer      Removes ~85M params (largest component in DNAFormer)\n",
      "2. NO Siamese          Single forward pass \n",
      "3. NO Fusion Module    Simpler output pipeline\n",
      "4. Fewer Kernels       3 instead of 4 kernel sizes\n",
      "5. Simpler Modules     Lighter alignment and embedding modules\n",
      "6. Linear Complexity   O(L) vs O(L) for self-attention\n",
      "7. nn.Embedding        Learnable embeddings instead of fixed one-hot\n",
      "\n",
      "MODULES REMOVED/SIMPLIFIED:\n",
      "\n",
      "- Transformer Encoder (12 layers, 32 heads)          BiGRU (2 layers)\n",
      "- Fusion Module (learnable vectors + 3 Conv1x1)      REMOVED\n",
      "- Kernel size 7 branch                               REMOVED\n",
      "- Second double_conv1D in each kernel branch         Single conv\n",
      "- Linear block in alignment (3 linear layers)        REMOVED\n",
      "- Output module (3 Conv1x1)                          Single Linear\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"ARCHITECTURAL DIFFERENCES SUMMARY\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "print(\"\"\"\n",
    "\n",
    "           Feature                       DNAFormer                 Compact BiGRU          \n",
    "\n",
    " Sequence Modeling                 12-layer Transformer        2-layer BiGRU              \n",
    " Attention Heads                   32                          N/A                        \n",
    " Model Dimension (d_model)         1024                        500 (embedding_filters)    \n",
    " Feedforward Dimension             2048                        N/A                        \n",
    " Alignment Kernels                 4 kernels {1,3,5,7}         3 kernels {1,3,5}          \n",
    " Alignment Structure               4 double_conv1D + linear    2 multi-kernel blocks      \n",
    " Embedding Structure               4 double_conv1D + linear    1 multi-kernel + linear    \n",
    " Siamese Architecture              YES (2 branches)            NO (single branch)         \n",
    " Fusion Module                     YES (vectors + 3 conv)      NO                         \n",
    " Output Module                     3 Conv1x1 layers            1 Linear layer             \n",
    " Input Encoding                    One-hot (4 channels)        nn.Embedding (300 dim)     \n",
    " Attention Complexity              O(L)                       O(L) linear                \n",
    " ~Parameters                       ~100M                       ~5-8M                      \n",
    "\n",
    "\n",
    "KEY ADVANTAGES OF COMPACT MODEL:\n",
    "\n",
    "1. NO Transformer      Removes ~85M params (largest component in DNAFormer)\n",
    "2. NO Siamese          Single forward pass \n",
    "3. NO Fusion Module    Simpler output pipeline\n",
    "4. Fewer Kernels       3 instead of 4 kernel sizes\n",
    "5. Simpler Modules     Lighter alignment and embedding modules\n",
    "6. Linear Complexity   O(L) vs O(L) for self-attention\n",
    "7. nn.Embedding        Learnable embeddings instead of fixed one-hot\n",
    "\n",
    "MODULES REMOVED/SIMPLIFIED:\n",
    "\n",
    "- Transformer Encoder (12 layers, 32 heads)          BiGRU (2 layers)\n",
    "- Fusion Module (learnable vectors + 3 Conv1x1)      REMOVED\n",
    "- Kernel size 7 branch                               REMOVED\n",
    "- Second double_conv1D in each kernel branch         Single conv\n",
    "- Linear block in alignment (3 linear layers)        REMOVED\n",
    "- Output module (3 Conv1x1)                          Single Linear\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "FINAL STATISTICS\n",
      "====================================================================================================\n",
      "\n",
      "Average Parameter Reduction: 96.7%\n",
      "Average FLOP Reduction:      96.0%\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Per-Dataset Summary:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Srinivasavaradhan:\n",
      "  DNAFormer: 103.40M params, 23.63G FLOPs\n",
      "  Compact:   3.41M params, 0.9567G FLOPs\n",
      "  Reduction: 96.7% params, 96.0% FLOPs\n",
      "\n",
      "Grass:\n",
      "  DNAFormer: 103.41M params, 25.19G FLOPs\n",
      "  Compact:   3.42M params, 1.0182G FLOPs\n",
      "  Reduction: 96.7% params, 96.0% FLOPs\n",
      "\n",
      "Erlich:\n",
      "  DNAFormer: 103.47M params, 33.09G FLOPs\n",
      "  Compact:   3.48M params, 1.3141G FLOPs\n",
      "  Reduction: 96.6% params, 96.0% FLOPs\n",
      "\n",
      "BinnedNanoporeTwoFlowcells:\n",
      "  DNAFormer: 103.40M params, 24.94G FLOPs\n",
      "  Compact:   3.43M params, 1.0206G FLOPs\n",
      "  Reduction: 96.7% params, 95.9% FLOPs\n",
      "\n",
      "BinnedTestIllumina:\n",
      "  DNAFormer: 103.40M params, 24.94G FLOPs\n",
      "  Compact:   3.43M params, 1.0206G FLOPs\n",
      "  Reduction: 96.7% params, 95.9% FLOPs\n"
     ]
    }
   ],
   "source": [
    "#### Final statistics\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"FINAL STATISTICS\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "avg_param_reduction = df_summary['Param_Reduction_%'].mean()\n",
    "avg_flop_reduction = df_summary['FLOP_Reduction_%'].mean()\n",
    "\n",
    "print(f\"\\nAverage Parameter Reduction: {avg_param_reduction:.1f}%\")\n",
    "print(f\"Average FLOP Reduction:      {avg_flop_reduction:.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Per-Dataset Summary:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for r in results_summary:\n",
    "    print(f\"\\n{r['Dataset']}:\")\n",
    "    print(f\"  DNAFormer: {r['DNAFormer_Params_M']:.2f}M params, {r['DNAFormer_FLOPs_G']:.2f}G FLOPs\")\n",
    "    print(f\"  Compact:   {r['Compact_Params_M']:.2f}M params, {r['Compact_FLOPs_G']:.4f}G FLOPs\")\n",
    "    print(f\"  Reduction: {r['Param_Reduction_%']:.1f}% params, {r['FLOP_Reduction_%']:.1f}% FLOPs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results saved to 'param_flops_comparison.csv'\n"
     ]
    }
   ],
   "source": [
    "# Export to CSV for paper\n",
    "df_summary.to_csv('param_flops_comparison.csv', index=False)\n",
    "print(\"\\nResults saved to 'param_flops_comparison.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PART 8: LaTeX Table Generation for Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "LaTeX TABLE FOR PAPER\n",
      "====================================================================================================\n",
      "\n",
      "\\begin{table}[t]\n",
      "\\centering\n",
      "\\caption{Parameter Count and FLOPs Comparison: DNAFormer vs. Compact BiGRU}\n",
      "\\label{tab:param_flops}\n",
      "\\begin{tabular}{lcccccc}\n",
      "\\toprule\n",
      "\\multirow{2}{*}{Dataset} & \\multicolumn{2}{c}{Parameters (M)} & \\multicolumn{2}{c}{FLOPs (G)} & \\multicolumn{2}{c}{Reduction (\\%)} \\\\\n",
      "\\cmidrule(lr){2-3} \\cmidrule(lr){4-5} \\cmidrule(lr){6-7}\n",
      " & DNAFormer & Ours & DNAFormer & Ours & Params & FLOPs \\\\\n",
      "\\midrule\n",
      "Srinivasavaradhan & 103.40 & 3.41 & 23.63 & 0.957 & 96.7 & 96.0 \\\\\n",
      "Grass & 103.41 & 3.42 & 25.19 & 1.018 & 96.7 & 96.0 \\\\\n",
      "Erlich & 103.47 & 3.48 & 33.09 & 1.314 & 96.6 & 96.0 \\\\\n",
      "BinnedNanoporeTwoFlowcells & 103.40 & 3.43 & 24.94 & 1.021 & 96.7 & 95.9 \\\\\n",
      "BinnedTestIllumina & 103.40 & 3.43 & 24.94 & 1.021 & 96.7 & 95.9 \\\\\n",
      "\\midrule\n",
      "Average & --- & --- & --- & --- & 96.7 & 96.0 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"LaTeX TABLE FOR PAPER\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "latex_table = r\"\"\"\n",
    "\\begin{table}[t]\n",
    "\\centering\n",
    "\\caption{Parameter Count and FLOPs Comparison: DNAFormer vs. Compact BiGRU}\n",
    "\\label{tab:param_flops}\n",
    "\\begin{tabular}{lcccccc}\n",
    "\\toprule\n",
    "\\multirow{2}{*}{Dataset} & \\multicolumn{2}{c}{Parameters (M)} & \\multicolumn{2}{c}{FLOPs (G)} & \\multicolumn{2}{c}{Reduction (\\%)} \\\\\n",
    "\\cmidrule(lr){2-3} \\cmidrule(lr){4-5} \\cmidrule(lr){6-7}\n",
    " & DNAFormer & Ours & DNAFormer & Ours & Params & FLOPs \\\\\n",
    "\\midrule\n",
    "\"\"\"\n",
    "\n",
    "for r in results_summary:\n",
    "    latex_table += f\"{r['Dataset']} & {r['DNAFormer_Params_M']:.2f} & {r['Compact_Params_M']:.2f} & \"\n",
    "    latex_table += f\"{r['DNAFormer_FLOPs_G']:.2f} & {r['Compact_FLOPs_G']:.3f} & \"\n",
    "    latex_table += f\"{r['Param_Reduction_%']:.1f} & {r['FLOP_Reduction_%']:.1f} \\\\\\\\\\n\"\n",
    "\n",
    "latex_table += r\"\"\"\\midrule\n",
    "Average & --- & --- & --- & --- & \"\"\"\n",
    "latex_table += f\"{avg_param_reduction:.1f} & {avg_flop_reduction:.1f} \\\\\\\\\"\n",
    "latex_table += r\"\"\"\n",
    "\\bottomrule\n",
    "\\end{tabular}\n",
    "\\end{table}\n",
    "\"\"\"\n",
    "\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "ANALYSIS COMPLETE!\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"ANALYSIS COMPLETE!\")\n",
    "print(\"=\" * 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
